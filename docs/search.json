[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "巫雨洋的学习日记",
    "section": "",
    "text": "This is a Learning Diary for CASA0023"
  },
  {
    "objectID": "WEEK1.html",
    "href": "WEEK1.html",
    "title": "1  WEEK1",
    "section": "",
    "text": "Sensors and remote sensing platforms\nSensors are devices that collect and record information about electromagnetic radiation capabilities and are mounted on remote sensing platforms. Sensors are classified according to how they work and into active and passive sensors. Remote sensing platforms are mainly divided into, ground, air and space based remote sensing platforms.\nElectromagentic waves\nElectromagnetic waves can be classified into different bands according to the order of their wavelength or frequency in a vacuum. The following diagram shows the electromagnetic spectrum commonly used in remote sensing.\n\n\n\nthe electromagnetic spectrum\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe magnitude of energy that an object reflects or emits across a range of wavelengths is called its spectral response pattern/spectral signatures. It can be used to identify objects.\n\n\nResolution\nIn remote sensing we refer to three types of resolution: spatial, spectral and temporal. Spatial Resolution refers to the size of the smallest feature that can be detected by a satellite sensor or displayed in a satellite image. It is usually presented as a single value representing the length of one side of a square. Remote Sensing » Spatial Analysis\n\n\n\nWe can use:\nCopernicus Open Accesss Hub to download RS images collected by Sentinel Series Satellite.\n\n\n\n\n\n\nNote\n\n\n\nFor Sentinel Series Satellite Overview\nPS. Very few of the links for additional information are in Chinese, as this makes it easier for me to understand.\n\n\nNote when selecting products, be careful about：\n\nSatellite transit period range (e.g. 2020/01/01 - 2020/03/07)\nData type (Sentinel1/2/3) Where Sentinel2 data can be downloaded in types S1C (unatmospherically corrected) and S2A (atmospherically corrected).\nCloud cover to extent [0 TO 5]\n\nWe can also use:\nLandsat data to download RS image from Landsat Series.\n\n\n\nCopernicus Open Accesss Hub\nGRANULE > sensor number > IMG_DATA > R10\nThen you can see the information about the bands.Including its resolution, central wavelength and description about which wave band it belongs to ( Blue/Green/Ted/VNIR/…)\nAnd here is a website which provides basic information from the characteristics of satellite to its products. For example ,this is for Landsat 5 (TM)\n\n\n\nQGIS\n\n\n\n\n\n\nTip\n\n\n\nTo process Sentinel data with QGIS, we can install Semi-Automatic Classification Plugin (SCP) version 7.\n\n\nWe can open monochrome images in different bands. We can also open the TCI image, which is a True Colour Image of B02 (Blue), B03 (Green), and B04 (Red) Bands.But the TCI has a range of pixel values from 0-255, it is not visually clear. We can get a more realistic level of colour by using BOA data.\n\n\n\n\n\n\nThe BOA bands\n\n\n\nIn the Sentinel 2 images available for download, you will find Level 1C data and Level 2A data. 1C data are orthorectified and geometrically refined atmospheric apparent reflectance products that are not atmospherically corrected. 2A images are orthorectified bottom-of-atmosphere (BOA) reflectance corrected images. What does this mean? Unlike Class 1C, Class 2A images correspond to atmospherically corrected images and provide reflectance data that are closer to the real thing (and therefore have a more realistic colour level). It is possible to distinguish between them visually (below), as a Class 2A image is sharper, has higher brightness and contrast, and does not show the whitish texture produced by atmospheric influences.\n\n\nIn addition, using the merge tool to merge single-band images can also obtain multi-band images. Then realize True Color Image by selecting the red, green and blue bands.\nSNAP\nSNAP is an application developed by the European Space Agency for processing Sentinel satellite data. We actually can open .zip file in SNAP which is very convenient. And if unzip the file and open each .tiff file separately, some important information will lose like the value of wavelength.This video can be help on how to open Sentinel data in SNAP.\nFollowing are some analysis we can do through the software:\n\nColour composites\n\nWe can achieve different band combinations by selecting different remote sensing image bands in RGB channels\n\n\n\n\n\n\nTip\n\n\n\nBe careful that different RS data may have different meanings for the band with the same No.\n\n\nFor Sentinel 2 data：\n\nSentinel 2 Bands and Combinations\n\n\n\n\n\n\n\nTypes\nRGB channels\ndescription\n\n\n\n\nNatural Color\nB4, B3, B2\ndisplay imagery the same way our eyes see the world\n\n\nColor Infrared\nB8, B4, B3\nemphasize healthy and unhealthy vegetation. denser vegetation is red,but urban areas are white\n\n\nAgriculture\nB11,B8,B2\nused to monitor the health of crops,highlighting dense vegetation that appears as dark green\n\n\nGeology\nB12, B11, B2\nfor finding geological features. This includes faults, lithology, and geological formations\n\n\n\nFor Landsat 5 TM data：\n\nLandsat 5 TM Bands and Combinations\n\n\n\n\n\n\n\nTypes\nRGB channels\ndescription\n\n\n\n\nNatural Color\nB3, B2, B1\ndisplay imagery the same way our eyes see the world\n\n\nStandard false color\nB4, B3, B2\nemphasize healthy and unhealthy vegetation. denser vegetation is red,but urban areas are white\n\n\nsimulated true color\nB7,B4,B3\nIdentify residential land and water bodies\n\n\n…\n…\n…\n\n\n\n\n\n\nColor Infrared\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn addition to band combination, we can also use some band calculations to obtain images that can highlight targets.For example, Vegetation Index (B8-B4)/(B8+B4), Moisture Index (B8A-B11)/(B8A+B11)\n\n\n\nImage statistics\n\nHistogram\nA graph that describes the relationship between each gray level in an image and its frequency of occurrence. Histograms for each band of the image can be viewed in SNAP. If we change the mapping range of image pixels in 0-255, the image will be changed visually. We generally omit 1% at the lower end and 4% at the upper end for mapping. In addition, image contrast can be improved by adjusting the histogram morphology.\n\n\n\n\n\n\nNote\n\n\n\nMean: reflects the overall brightness of the image\nVariance: refers to the variance of the brightness value of each band, reflecting the volume of the information\n\n\nScatterplot\nThe scatterplot is used to analyze the spatial distribution relationship between two bands. For example, draw a scatter diagram of red (vegetation absorbs) and Near-infrared (NIR, that vegetation strongly reflects). Since higher NIR values and lower red values indicate dense vegetation, while lower values of both are usually bare soil. Therefore, the image can roughly analyze the distribution characteristics of different ground objects.\n\n\n\nScatter Plot\n\n\n\nMasking and resampling\n\nRemote sensing images can be cropped using vector data (Raster > Masks > Land/Sea mask) to leave areas of interest. Note that SNAP can only read ESRI shapefiles. In addition, before cropping, we need to unify the resolution of the band, you can use the resampling tool (Raster > Geometric > Resampling)\n\nComparison of spectral signatures\n\nWe can extract the spectral signatures of different land cover classes using the Semi-automatic Classification plugin in QGIS 3.16. Here is a tutorial video\n\n\n\nSpectral Signatures in QGIS\n\n\nAlso, it is easy to do it in SNAP. Basically you just need to use Optical->Spectrum View, and make sure that your image has band information. Besides, you can use Pin tool to make sample, and compare the curve of different objects. Here is another tutorial video about Pins and Spectrum Tool in SNAP\n\n\n\nSpectrum View in SNAP"
  },
  {
    "objectID": "WEEK1.html#application",
    "href": "WEEK1.html#application",
    "title": "1  WEEK1",
    "section": "1.2 Application",
    "text": "1.2 Application\nThe main application of Remote Sensing Image is about recognizing geographical entities, which is well used in agriculture, forestry, geology, marine, meteorology, hydrology, military, environmental protection and other fields.Here, I will give same simple example of how we use remote sensing image in agriculture scenarios.\n\n\n\nUsing RS to predict corn harvest time, Source: Janoušek et al. 2021\n\n\nCrop remote sensing production estimation is based on the collection of different spectral signatures of various crops in different growth stages according to biological principles, through the surface information recorded by the sensor on the platform, to identify crop types, monitor crop growth, and predict crop yield before crop harvest a range of methods. This technology can dynamically monitor the growth process of crops, measure the planting area, estimate the output per unit area and estimate the total output.\nTraditionally, optical and infrared sensors will be used to detect the irrigated areas, for example, Thenkabail et al. (2005) developed a comprehensive algorithm based on timeseries of MODIS spectral bands (2, 3, 5, 6, and 7) data to detect irrigation and rainfed classes along with crop onset, peak, and senescence (aging of the plant).\nAnd for crop yield assessment, using remote sensing of plant photosynthetic activity is a good way as photosynthetic activity influences biomass production. Both passive and active microwave remote sensing can help us to do it. There are some indicators or models like the Vegetation Optical Depth (VOD) (Guan et al. 2016). Besides, regression relationship with indices such as NDVI, RVI, and VCI are used to estimate crop yield.\n\n\n\nSource: Tolomio et al. 2020\n\n\nIn addition, remote sensing technology can also be useful when detecting agricultural pests and diseases. Indices such as Disease Water Stress Index (DWSI) , Disease Index (DI) and Yellow Rust Index (YRI) for detecting wheat yellow rust, Aphid Index (AI), and Leafhopper Index (LHI) , among others are proposed for pest and disease detection (Karthikeyan et al. 2020)."
  },
  {
    "objectID": "WEEK1.html#reflection",
    "href": "WEEK1.html#reflection",
    "title": "1  WEEK1",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\n\nFor remote sensing image data, the products generated by different sensors are very different, such as the information of bands and the structure of downloaded files. This requires us to have a good understanding of these video products before processing the data, otherwise you don’t even know how to open them in the software.\nAs for software, these software are not very friendly to novices, such as densely packed menu bars and tools that don’t know where they are. Fortunately, YouTube has many tutorials to help you operate.\nThe subject of remote sensing involves some physical knowledge, such as spectrum and radiation, which is sometimes a bit difficult to understand. The remote sensing of the visible light part can be interpreted through eyes which is helpful to understand. Others, such as microwave remote sensing, are even more difficult to understand.\nHowever, when I was studying agricultural remote sensing, I felt the power of this technology. When the study area is large, remote sensing is a good choice to monitor dynamic changes. Moreover, it can digitize the characteristics of geographic entities, such as the reflectivity of the electromagnetic spectrum with different wavelengths. This digitization provides opportunities for further data analysis (and the scale of the data is easily consistent, this is great). Therefore, a large number of ML and AI technologies are applied to remote sensing image processing. This brings greater promise to the application of remote sensing."
  },
  {
    "objectID": "WEEK1.html#reference",
    "href": "WEEK1.html#reference",
    "title": "1  WEEK1",
    "section": "1.4 Reference",
    "text": "1.4 Reference\n        Janoušek, Jiří, et al. “Using UAV-based photogrammetry to obtain correlation between the vegetation indices and chemical analysis of agricultural crops.” Remote Sensing 13.10 (2021): 1878.\n        Grant, J. P., et al. “Comparison of SMOS and AMSR-E vegetation optical depth to four MODIS-based vegetation indices.” Remote Sensing of Environment 172 (2016): 87-100.\n        Thenkabail, Prasad S., Mitchell Schull, and Hugh Turral. “Ganges and Indus river basin land use/land cover (LULC) and irrigated area mapping using continuous streams of MODIS data.” Remote Sensing of Environment 95.3 (2005): 317-341.\n        Karthikeyan, L., Ila Chawla, and Ashok K. Mishra. “A review of remote sensing applications in agriculture for food security: Crop growth and yield, irrigation, and crop losses.” Journal of Hydrology 586 (2020): 124905.\n        Huang, Jianxi, et al. “Assimilation of remote sensing into crop growth models: Current status and perspectives.” Agricultural and Forest Meteorology 276 (2019): 107609.\n        Tolomio, Massimo, and Raffaele Casa. “Dynamic crop models and remote sensing irrigation decision support systems: A review of water stress concepts for improved estimation of water requirements.” Remote Sensing 12.23 (2020): 3945.\nAnd some other website may be useful: Learn GIS and Geography"
  },
  {
    "objectID": "WEEK2.html",
    "href": "WEEK2.html",
    "title": "2  WEEK2",
    "section": "",
    "text": "1 2 3 4 5"
  },
  {
    "objectID": "WEEK3.html",
    "href": "WEEK3.html",
    "title": "3  WEEK3",
    "section": "",
    "text": "sensor and image\n\n\n\n\nHere shows a basic flow of how remote sensing is used in practice, it includes a few steps from data collection. As there are many factors influencing the RS image acquisition like, sensor characteristics, weather (particularly cloud cover), acquisition method (satellite or airborne) and others, remotely sensed images can contain flaws within them. Therefore, to improve image quality as a basis for further analysis, the most common step is carrying out corrections.\n\n\n\n\nflowchart LR\n  A(Acquisition) --> B(Pre-processing)\n  B --> C(Image Analysis)\n  C --> D(Products)\n  D --> E(Application)\n\n\n\n\n\n\n\n\n\nTo talking about this processing chain we can introduce the product levels. And we will summary some main points about each correction.\n\n\n\ncorrection and product levels\n\n\nThese are some website helpful to understand each process,\nThe Importance of Radiometric Calibration\nGeometric Corrections in Remote Sensed Image\nWhat is Atmospheric Correction in Remote Sensing?\nand following are some summaries.\nRadiometric Calibration\nRadiometric Calibration refers to the ability to convert the digital numbers recorded by satellite imaging systems into to spectral radiance, reflectance, or brightness temperatures.\nThe purpose is to eliminate the error of the sensor itself and determine the accurate radiation value at the entrance of the sensor.\nMethods include laboratory calibration, on-board/satellite calibration, and site calibration. Note that different sensors have different radiation calibration formulas.\n\n\n\n\n\n\nHow to tell if your imagery is lacking a radiometric workflow:\n\n\n\nWithout radiometric calibration, you may see the following effects:\n\nUnderexposed images, especially surrounding bright objects on the landscape\nIrregular coloration\nIndex values, such as NDVI, that appear to change dramatically and unexpectedly near roads or buildings\nExtreme banding or patchiness in your mosaic\n\n\n\nGeometric Correction\n\nCauses\n\nCurvature of the earth\n\nTopography of the terrain\n\nEarth rotation\n\nView angle (off-nadir)\n\n…\n\nCorrection Steps\n\nAcquisition of ground control points (GCP) : Use Ground Control Points (GCP) as a reference point for correct location data\nBuilding a Sensor Model: formulate the relation between the current and the reference location data\nCoordinate Transformation: Once a suitable sensor model is defined it can be applied to the image raw data to calculate the corrected coordinates of the pixel locations\nResampling of Pixel Values: The coordinate transformations shifts and rotates the pixel locations. These new locations do not fit into a regular grid of the raster files. Therefore, the pixel values need to be resampled to assign each grid cell one raster values.\n\n\n\n\n\n\nWhat is a ground control point (GCP)?\n\n\n\nGround Control Points (GCPs) are defined as points on the surface of the earth of known location used to geo-reference Landsat Level-1 data. GCPs are updated as needed to continually improve Landsat data. GCPs can be downloaded and used as reference data.\n\n\n\nExamining the Accuracy\n\nRoot mean squared error can be used to measure the difference between locations that are known and locations that have been interpolated or digitized or resampled. \\[\\begin{equation}\nRMS_{error}=\\sqrt{(X^{'}-X_{orig}+Y^{'}-Y_{orig})^2}\n\\end{equation}\\]\nAtmospheric Correction\nremoves the scattering and absorption effects from the atmosphere to obtain the surface reflectance characterizing (surface properties)\n\nCauses\n\nAtmospheric scattering\nAtmospheric reflections\nTopographic attenuation\n\n…\n\nCorrection Methods\n\nMethods can be divided into 2 types according to the results of the correction:\n\nAtmospheric Correction\n\n\n\n\n\n\nType\nNotes\n\n\n\n\nRelative atmospheric correction method\nfor example the dark object subtraction (DOS), histogram adjustment and psuedo-invariant features (PIFs)\n\n\nAbsolute atmospheric correction method\nthere are some transfer models or codes such as MODTRAN 4+ and 6S can be used to deal with scattering and absorption, this is called model-based atmospheric correction methods. We can also use the empirical line correction by take measurements in site using a field spectrometer.\n\n\n\nOrthorectification Correction\northorectification is the process of improving the horizontal accuracy of imagery.\n\nCauses\n\nCamera and sensor orientation\nTopographic relief displacement\nEarth curvature\n\n…\n\nCorrection Methods\n\nWe can use some orthorectification algorithms to deal with the distortion, also, softerwares like QGSIS and SAGA GIS can be helpful.\n\n\n\n\n\n\nOrthorectification vs Georectification\n\n\n\nGeorectify take an image that has not been adjusted to be in a known coordinate system, and put it into a known coordinate system.\nOrthorectify take an image in its original geometry and very accurately adjust it so that it is in a known coordinate system, with distortions due to topographic variation corrected.\n\n\n\n\n\n\nImage enhancement algorithms are applied to remotely sensed data to improve the appearance of an image for human visual analysis or occasionally for subsequent machine analysis. There is no such thing as the ideal or best image enhancement because the results are ultimately evaluated by human.\n\nImage enhancement is attempted after the image is corrected for geometric and radiometric distortion. here we will summarize some of the most popular enhancements techniques.\nContrast Enhancement\n\nWhy it is needed\n\nMost of the satellite images lack adequate contrast with nearly uniform tones of gray.This results in difficulty in interpret the image. Reasons for low contrast of image can be various. For example, the objects may have a nearly uniform electromagnetic response at the wavelength band of energy, scattering of electromagnetic energy by the atmosphere can reduce the contrast of a scene…\n\nPrinciple\n\nContrast enhancement techniquest (also referred to as contrast stretching) expand the range of brightness values in an image so that the image can be efficiently displayed in a manner desired by the analyst.\n\nMethods/Examples\n\nContrast enhancement techniques can be divided into 2 categories.\n\nContrast Enhancement\n\n\nType\nNotes\n\n\n\n\nLinear Contrast Enhancement\nincluding Minimum-Maximum Contrast Stretching, Percentage Linear and Standard Deviation Stretching,Piecewise Linear Contrast Stretching. They differ in the Stretching Function. Like the picture below shows how Minimum-Maximum Contrast Stretching works.\n\n\nNonlinear contrast enhancement stretching\none applied of the most useful enhancements is histogram equalization. In this technique, histogram of the original image is redistributed to produce a uniform population density. The whole process can be a little complex, which can be found here at Page 8\n\n\n\n\n\n\nLinear contrast stretch versus histogram equalization\n\n\nBand Ratioing\n\nWhy it is needed\n\nSometime differences in brightness value from identical surface materials are caused by topographic slope and aspect, shadow, or seasonal change in sun illumination angle and intensity. These conditions may hamper ability of interpreter or classification algorithm to identify correctly surface materials or land use in a remotely sensed image. We can use Band Ratios to enhance the spectral differences between bands and to reduce the effects of topography.\n\nPrinciple\n\nBasically, Band Ratioing is about doing band algebra, dividing one spectral band by another produces an image that provides relative band intensities.The ratio is the numerator divided by the denominator\n\nMethods/Examples\n\nThere are many defined indexes show us the way to do band ratioing, such as The Normalised Difference Vegetation Index (NDVI) which is based on the fact that healthy and green vegetation reflects more in the NIR but absorbs in the Red wavelength. \\[\\begin{equation}\nNDVI=\\frac{NIR-Red}{NIR+Red}\n\\end{equation}\\]\nThere are many other ratios, all of which are detailed on the Index Database\n\n\n\nThe sunlight causes the different value of land cover, but the ratio is constant\n\n\nSpatial filtering\n\nA characteristic of remotely sensed image is a parameter called spatial frequency; defined as the number of changes in brightness value per unit distance for any particular part of an image.\n\n\nWhy it is needed\n\nSpatial filters serve a variety of purposes, such as detecting edges along a specific direction, contouring patterns, reducing noise, and detail outlining or smoothing. Filters smooth, sharpen, transform, and remove noise from an image so that you can extract the information you need.\n\nPrinciple\n\nSpatial filtering applies the idea of convolution, which is an algorithm that consists of recalculating the value of a pixel based on its own pixel value and the pixel values of its neighbors weighted by the coefficients of a convolution kernel. Basically, to acheive Spatial Filtering we firstly decide a window, that is the size of the convolution kernel(3 × 3, 5 × 5…can be either square or rectangular). And according to different types of convolutions:gradient, Laplacian, smoothing, and Gaussian, different convolution kernel contents or the weight will be used to calculated the new value of the central kernel pixel.\nFrom the perspective of mathematical calculation, it is to construct a matrix, multiply the elements of the matrix by the value of the corresponding pixel, and then add up the products as the new value of the central pixel. Like the picture blew illustrates.\n\n\n\nExample of Spatial filtering\n\n\n\nMethods/Examples\n\nSpatial filters fall into two categories:\n\nSpatial filtering\n\n\n\n\n\n\nType\nNotes\n\n\n\n\nHighpass filters\nemphasize significant variations of the light intensity usually found at the boundary of objects. Highpass frequency filters help isolate abruptly varying patterns that correspond to sharp edges, details, and noise.\n\n\nLowpass filters\nattenuate variations of the light intensity. Lowpass frequency filters help emphasize gradually varying patterns such as objects and the background. They have the tendency to smooth images by eliminating details and blurring edges.\n\n\n\nSpecific tye of Spatial Filter and its application can be found in here\nThere are also other way to process the image and gain information we needed, like Texture, Data fusion and PCA, here I am trying to use R language to achieved them. In addition, paying special attention on them in the application part.\n\n\n\nHere I will use R to achieve some enhancement methods. Firstly we use pull() to load our raster layers into a stack, then we can easily pick out one of the band.\n# List your raster files excluding band 8 using the patter argument\nm1<-dir_info(here::here(\"LC09_L2SP_203023_20220326_20220328_02_T1\")) %>%\n  dplyr::filter(str_detect(path, \"[1B23456790].TIF\")) %>%\n  dplyr::select(path) %>%\n  pull() %>%\n  as.character() %>%\n  # Load our raster layers into a stack\n  terra::rast()\nreferring the equation given above, the NDVI can be calculated quickly,\nm1_NDVI <- (m1$LC09_L2SP_203023_20220326_20220328_02_T1_SR_B5 - m1$LC09_L2SP_203023_20220326_20220328_02_T1_SR_B4 ) / (m1$LC09_L2SP_203023_20220326_20220328_02_T1_SR_B5 + m1$LC09_L2SP_203023_20220326_20220328_02_T1_SR_B4)\n\n\n\nFigure of NDVI >0.2 the high value stands for the healty plant\n\n\nand the glcm package can help us to do the Texture Analysis.\nglcm(x, n_grey = 32, window = c(3, 3), shift = c(1, 1), statistics = c(\"mean\", \"variance\", \"homogeneity\", \"contrast\", \"dissimilarity\", \"entropy\", \"second_moment\", \"correlation\"), min_x=NULL, max_x=NULL, na_opt=\"any\", na_val=NA, scale_factor=1, asinteger=FALSE)\n#statistics means a list of GLCM texture measures to calculate\nif we usd the homogeneity measure the results is shown blow\n\n\n\nthe homogeneity of the image\n\n\n\n\n\n\n\n\nHomogeneity texture measure\n\n\n\nHomogeneity was a measure of the similarity of pixel values in the neighborhood defined by the processing window, ranging from 0.0 (completely dissimilar) to 1.0 (all cells having equivalent values) (Lane CR et al. 2014)\n\n\nthe video Introduction to textural classification in QGIS 3.10 shows basic concept of Texture Analysis as well as how to do it in QGIS.\nFor PCA, rasterPCA can be helpful to reduce the dimensionality of our data."
  },
  {
    "objectID": "WEEK3.html#application",
    "href": "WEEK3.html#application",
    "title": "3  WEEK3",
    "section": "3.2 Application",
    "text": "3.2 Application\nTexture Analysis in Classification\nIn remote sensing image, texture means the spatial variation of the pixels in an small area. So, it is different from what the image visually looks like. Coburn, C. A., & Roberts, A. C. (2004) pointed out that of all the spatial information that can be extracted from remotely sensed data, texture may be the most useful for segmenting images. Therefore, Texture Analysis can provide information that the visual interpretation cannot. when we are trying to distinguish the geographic entities or the land cover. Here is a example about how texture analysis can be combined with Random Forest technique to accurately differentiate land covers of urban vegetated areas.\nFeng Q et al. (2015) achieved urban vegetation mapping by using UAV Remote Sensing data. Its whole processes include 3 parts:data acquisition and preprocessing;image classification; accuracy analysis. The Random Forest and texture analysis were introduced in the second part.\nFor the texture analysis, six least correlated texture measures according to Szantoi Z et al. (2013) were used in the study(mean (MEA), standard deviation (STD), homogeneity (HOM), dissimilarity (DIS), entropy (ENT) and angular second moment (ASM)). Besides, only Green band was used for the texture statistics and they were second-order (co-occurrence) textures.\n\n\n\nThe workflow of the study\n\n\n\n\n\n\n\n\nFirst-Order (Occurrence) and Second-Order (Co-Occurrence)\n\n\n\n\nFirst-order metrics operate on the counts, or occurrences, of the different digital number (DN) values within a kernel. Separate images with different spatial arrangements of the same pixels within a single kernel will yield the same first-order texture value in both kernels.\nSecond-order (also called co-occurrence) metrics analyze the relationship between pixel pairs. They use a co-occurrence matrix to calculate texture values. This matrix is a function of both the angular relationship and distance between two neighboring pixels. It shows the number of occurrences of the relationship between a pixel and its specified neighbor.\n\nfind more at Texture Metrics Background\n\n\nHere, we skip the steps the author calculated the textures, just remember that 9 texture window sizes were chosen and texture features derived at different window sizes were added separately as additional ancillary bands to the RGB images for further classification.\nWe will not talk about Random Forest too much here, it is a supervised classification and the UAV orthophotos and ancillary texture features were inputs.\nHere is the figure in the paper which shows 2 original RGB images. According to field survey and the aim of the project, three vegetated land covers were chosen as follows: grass, trees and shrubs\n\n\n\nThe original RGB images\n\n\nAnd here is the results of classification. For the picture below, the GRB-only means only used GRB bands as inputs while the RGB+GT31 means the texture metrics with the moving windows 31 × 31 (GT31) included. So, this figure depicts the differences before and after the inclusion of texture features. We can easily see that both Image-A and Image-B show some “salt and pepper” effects.And the inclusion of texture improves the classification accuracy from a visual point of view. This is because the inclusion of texture increases the between-class separability and tends to remove small isolated groups of pixels.\n\n\n\nThe classification results.(a) Image-A RGB-only; (b) Image-A RGB+GT31; (c) Image-B RGB-only; (d) Image-B RGB+GT31\n\n\nIn addition, if looking at the details, we can see that some part of the area with tree, grass and bare soil come together, some pixels are misclassified in RGB-only classification. This may due to tree, dead shrubs and bare soil share similar colors. But these misclassifications can be improved after the inclusion of six texture features at window size 31.\nIn conclusion, this study manifests that the including of texture features can increase the classification accuracy when differentiating land covers of urban vegetated areas. It also shows us the impact of texture window size on classification accuracy, which turns out has an inverted U relationship.\nHowever, personally speaking, I think there are still some points that the author hasn’t provided enough information. For example, why the Second-order metrics was chosen here instead of the First-Order one? Will there be any difference of the classification results if we use the latter? In addition, I am not very sure about why the experiment used 6 texture measures. Does the contribution of these texture measures to the classification make a difference?"
  },
  {
    "objectID": "WEEK3.html#reflection",
    "href": "WEEK3.html#reflection",
    "title": "3  WEEK3",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\n\nAs can be seen from the flow chart and product levels diagram in 3.1.1, there is still a long data processing process between the creation of a remote sensing image by the sensor and the use of it to solve practical problems. This shows that remote sensing is not equal to taking a picture of the ground in a very distant place. The study of correction has deepened my understanding of electromagnetic radiation and the influence of different factors on radiation. From the perspective of problem solving, it allows us to clearly understand what happens to different radiations from generation to acceptance by the sensor. For example, Radiometric Calibration means that the sensor itself may have errors, Geometric Correction shows the influence of topography of the terrain on the image, and Atmospheric Correction shows the weakening and scattering by the atmosphere in the entire electromagnetic wave radiation process.\nImages are a way of expressing information, and remote sensing images are no exception. For ordinary images, we can use the components of the image space for interpretation, that is, interpretation based on vision. For example, vegetation is green and water is blue. Therefore, on the one hand, the contrast enhancement of remote sensing images can expand the difference between pixel value of objects, which is very beneficial to image interpretation. In many cases, we will adjust the histogram to highlight the features we want to observe. In addition, the segmented gray-scale stretching can target the dark and bright parts of the image to suppress low-frequency parts or enhance high-frequency parts as you decide.\nHowever, the land use distribution of real scenes is often complex, and when the surface area of the image cover is large, it is difficult to distinguish geographical entities only by the difference in hue. The misclassified situation will happen especially in complex terrain (such as mountainous terrain). At this time, the importance of spatial structure information is highlighted. Image texture is useful for the separation of different categories of regions, which is related to the visual perception of roughness or smoothness of image features. A large number of studies have shown that adding texture information as a band to image classification is conducive to improving the accuracy of the classification structure. This reflects the advantage of image enhancement processing, that is, the ability to obtain spatial information.\nFurthermore, in the actual process, some difficulties may be encountered in image enhancement processing. For example, when the image quality is not good, some noise may also be enhanced. So this is like a double-edged sword. When using an enhancement algorithm, you should consider distinguishing between useful information and noise in the image. Ali et al. (2001) pointed out that for noisy remote sensing images, different edge detection operators detect different edge maps. Some of these algorithms do not perform well. This means that we need to be careful when doing image enhancement, otherwise we may go the wrong way。"
  },
  {
    "objectID": "WEEK3.html#reference",
    "href": "WEEK3.html#reference",
    "title": "3  WEEK3",
    "section": "3.4 Reference",
    "text": "3.4 Reference\n        Feng, Q., Liu, J., & Gong, J. (2015). UAV remote sensing for urban vegetation mapping using random forest and texture analysis. Remote sensing, 7(1), 1074-1094.\n        Coburn, C. A., & Roberts, A. C. (2004). A multiscale texture analysis procedure for improved forest stand classification. International journal of remote sensing, 25(20), 4287-4308.\n        Szantoi, Z., Escobedo, F., Abd-Elrahman, A., Smith, S., & Pearlstine, L. (2013). Analyzing fine-scale wetland composition using high resolution imagery and texture features. International Journal of Applied Earth Observation and Geoinformation, 23, 204-212.\n        Lane, C. R., Liu, H., Autrey, B. C., Anenkhonov, O. A., Chepinoga, V. V., & Wu, Q. (2014). Improved wetland classification using eight-band high resolution satellite imagery and a hybrid approach. Remote sensing, 6(12), 12187-12216.\n        Ali, M., & Clausi, D. (2001). Using the Canny edge detector for feature extraction and enhancement of remote sensing images. IEEE International Symposium on Geoscience and Remote Sensing (IGARSS), 5, 2298-2300."
  },
  {
    "objectID": "WEEK4.html",
    "href": "WEEK4.html",
    "title": "4  WEEK4",
    "section": "",
    "text": "The learning diary of this week is about policies in cities, and how we can use RS technology to achieve the goals/ideas behind the policy. And here is an example in Pau da Lima, City of Salvador, Brazil."
  },
  {
    "objectID": "WEEK4.html#policy",
    "href": "WEEK4.html#policy",
    "title": "4  WEEK4",
    "section": "4.1 Policy",
    "text": "4.1 Policy\nIn this case study, the NEW!! World Cities Report 2022 is the intact referring policy. And the Chapter 10: Building Resilience for Sustainable Urban Futures (Page.334) is the section particularly been focused on. There are some of the Policy points mentioned in this part:\n\n\nInvesting in key urban infrastructure must be a prerequisite for building sustainable and resilient urban futures.\nPolicymakers must match urban risk assessments with appropriate solutions\nVisioning and implementation of urban resilience plans must prioritize the poorest and most vulnerable communities.\nBuilding urban resilience will not succeed without public participation.\n\n\nSo the infrastructure, urban risk, communities in disadvantage are highlighted here. It reminds us the vulnerability of slums and informal settlements due to the poor infrastructure is far from resilience.\nFollowing the report, in 10.4. Environmental Resilience, it talks about raising awareness of different local urban risks and identification of feasible disaster prevention and preparedness. In which the discussion of flood is involved:\n\n…focused principally on flood risk, with imaginative awareness raising strategies and mitigation strategies including a shift from traditional hard engineering solutions towards naturebased solutions and ecosystem services such as restoring and expanding riverine vegetation and floodplains…\n\nAnd next, the report emphasizes on building resilience in slums and informal settlements, this is because people and areas there are facing particular vulnerabilities and high risks.\nTherefore, referring the policy, this case study tried to pay attention to building resilience in slums and informal settlements. Focused principally on flood risk. The policy suggests that we could achieve the development of resilient cities with:\n\n…imaginative awareness-raising strategies and mitigation strategies including a shift from traditional hard engineering solutions towards nature-based solutions and ecosystem services…\n\nThat is quite abstract. However, as RS technology is closely related to the nature, and can be used in both hard engineering and software system, I think we can achieve it."
  },
  {
    "objectID": "WEEK4.html#study-area-and-the-problem-desription",
    "href": "WEEK4.html#study-area-and-the-problem-desription",
    "title": "4  WEEK4",
    "section": "4.2 Study Area and the Problem Desription",
    "text": "4.2 Study Area and the Problem Desription\n(Slum community) Pau da Lima, City of Salvador, Brazil\n\n\n\nPau da Lima\n\n\nIn total, 67% of the population of Salvador and 37% of the urban population in Brazil reside in slum communities with equal or greater levels of poverty as that found in Pau da Lima. And a tropical rainforest country, the average monthly precipitation is more than 60 mm, which shows that the area has a large amount of precipitation all year round. However, the area’s drainage system is unstable and has open sewers, so overflows often occur during periods of heavy rain, and the valley floor is prone to flooding. Sanitary conditions in the area are generally poor, and residents are often exposed to garbage and sewers. Besides, the buildings in slum areas are informal, so their structure is often unstable which will cause danger in storm rain periods."
  },
  {
    "objectID": "WEEK4.html#proposed-measures",
    "href": "WEEK4.html#proposed-measures",
    "title": "4  WEEK4",
    "section": "4.3 Proposed Measures",
    "text": "4.3 Proposed Measures\n\n4.3.1 Assessment\n\nTargets and Methods for Flood Risk Assessment\n\n\n\n\n\n\n\n\nName\nNotes\nAims\nData and Methods\n\n\n\n\nLand-use and Land-cover recognition\nThe complex distribution of buildings in slum areas and the lack of a clear building plan make it difficult to map the areas. Furthermore, the Pau da Lim slum is in a valley area and is heavily vegetated, which makes it more difficult to classify land use\nIdentify the distribution of settlements, reservoirs, sewers, etc. in slums\nUsing multispectral or hyperspectral imagery, combined with image recognition and machine learning algorithm techniques; Using SAR for texture analysis to identify residential buildings\n\n\nTerrain detection\nFor houses in low-lying areas and in flood-prone areas, the risk factor is higher in flooding\nAssessment of the hazard index at the terrain level\nUsing LIDAR technology to obtain topographic elevations and generate DEM data\n\n\nPhysical safety of buildings measurement\nMany buildings in slum areas are not built to code and have unstable roofs, which can easily fall off during heavy rainfall and cause injuries\nDetection of dangerous buildings\nUsing VHR images to extract physical, geometric features of buildings\n\n\nEnvironmental Assessment\nPoor hygiene around the house, such as sewage running through or open piles of rubbish, can cause serious health problems when flooding\nAnalysis of the environmental health of the surroundings of the residence\nMonitoring of water quality parameters (e.g. water temperature, cyanobacteria concentration, chlorophyll concentration, etc.) in water bodies using meteorological satellites for effluent identification\n\n\n\n\n\n4.3.2 Reconstruction\nReferring to the results of the assessment, dangerous houses are rebuilt and houses in flood-prone areas unsuitable for building houses are relocated. In addition, the layout of sewerage pipes can be adapted to the land use situation, etc.\n\n\n4.3.3 Monitoring\n\nTargets and Methods for Monitoring Flood\n\n\n\n\n\n\n\nName\nAims\nData and Methods\n\n\n\n\nPrecipitation monitoring\nReal-time monitoring of the relationship between soil moisture content and rainfall in the area to predict the occurrence of flooding\nUsing the Advanced SCATterometer (ASCAT) on MetOp for soil moisture observations, combined with the SM2RAIN-ASCAT global-scale rainfall product dataset for flooding predictions\n\n\nReal-time detection of flooding disasters\nIdentification of the area currently affected by flooding\nUsing Landsat5TM data and the Normalised Difference Water Index (NDWI) for the extraction of water bodies. If the weather is bad, SAR can also be used too\n\n\nDisaster assessment\n\nComparing remote sensing images before and after flooding\n\n\nLand use change analysis\nAnalysis of the expansion of the study area and adjustment of the drainage system to the changes in land use\nLike the data and methods of Land-use and Land-cover recognition"
  },
  {
    "objectID": "WEEK4.html#summary",
    "href": "WEEK4.html#summary",
    "title": "4  WEEK4",
    "section": "4.4 Summary",
    "text": "4.4 Summary\nFrom understanding, to prevention, and to handling"
  },
  {
    "objectID": "WEEK4.html#reference",
    "href": "WEEK4.html#reference",
    "title": "4  WEEK4",
    "section": "4.5 Reference",
    "text": "4.5 Reference\n        Munawar, H. S., Hammad, A. W. A., & Waller, S. T. (2022). Remote Sensing Methods for Flood Prediction: A Review. Sensors (Basel, Switzerland), 22(3), 960. https://doi.org/10.3390/s22030960\n        Kuffer M, Pfeffer K, Sliuzas R. Slums from Space—15 Years of Slum Mapping Using Remote Sensing. Remote Sensing. 2016; 8(6):455. https://doi.org/10.3390/rs8060455\n        Reis, R. B., Ribeiro, G. S., Felzemburgh, R. D., Santana, F. S., Mohr, S., Melendez, A. X., … & Ko, A. I. (2008). Impact of environment and social gradient on Leptospira infection in urban slums. PLoS Neglected Tropical Diseases, 2(4), e228. https://doi.org/10.1371/journal.pntd.0000228\n        Klemas, V. (2015). Remote sensing of floods and flood-prone areas: An overview. Journal of Coastal Research, 31(4), 1005-1013.https://doi.org/10.2112/JCOASTRES-D-14-00160.1"
  },
  {
    "objectID": "WEEK4.html#summary-and-reflection",
    "href": "WEEK4.html#summary-and-reflection",
    "title": "4  WEEK4",
    "section": "4.4 Summary and Reflection",
    "text": "4.4 Summary and Reflection\n\nFrom understanding, preventing, and to handling the flood disaster in slum areas, this case study shows the possibility that RS technology can be helpful when constructing a resilient city.\nFor the whole process, this study is about measuring the risk level of each area. Before the flood, planning to reconstruct the infrastructure like the drainage system, remove settlements in dangerous areas, etc, so we can defend against the flood better and minimize the damage it causes. And when the flood is coming, we hope there is a system that can predict its accuracy, giving people adequate reaction time. This would be with the help of a monitoring system. In addition, during and after the flood, the system can provide information on the impact of disasters in different regions. This is especially helpful for rescue and re-planning in the future.\nThe advantage of RS is its real-time response to environmental changes. This is crucial when dealing with issue associated with nature. Besides, as a technology based on data, it can well applied in different system, playing a role of information provider.\nHowever, there are also certain challenges in applying RS in this case. The most nonnegligible and realistic point is the investment of money and professionals. Although urban issues and slum issues both involve geographic space and the social activities of people, the former are often considered worthy of costly long-term research. For the latter, a large number of existing slum rescue projects are based on “demolition” and “reconstruction”, so whether it is reasonable to invest a lot of manpower and money to build a RS system for an object that only exists for a limited time needs to be considered by the local government.\nOf course, one solution is to build a flexible system that can be used in more than one place, no matter in downtown area or in slum. And this will face another major challenge, that is, the huge differences of areas.This includes, but is not limited to climate, topography, architectural structure and the stage of development of the city. Therefore, we need a large number of RS-based practices, so that we can provide references this kind of projects in the future.\nAnd last, for the policy. I think most of the policies show a real ideal and satisfactory planning for the future city while some of them are very vague. if we can catch the essence of the policy, we may come up with measures that can achieve the goal. For example, the resiliency of the city seems very abstract. But if we are talking about cities responding to natural disasters quickly and effectively, then it will be clear what we suppose to do."
  }
]