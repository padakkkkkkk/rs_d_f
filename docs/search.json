[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "巫雨洋的学习日记",
    "section": "",
    "text": "This is a Learning Diary for CASA0023"
  },
  {
    "objectID": "WEEK1.html",
    "href": "WEEK1.html",
    "title": "1  WEEK1",
    "section": "",
    "text": "Sensors and remote sensing platforms\nSensors are devices that collect and record information about electromagnetic radiation capabilities and are mounted on remote sensing platforms. Sensors are classified according to how they work and into active and passive sensors. Remote sensing platforms are mainly divided into, ground, air and space based remote sensing platforms.\nElectromagentic waves\nElectromagnetic waves can be classified into different bands according to the order of their wavelength or frequency in a vacuum. The following diagram shows the electromagnetic spectrum commonly used in remote sensing.\n\n\n\nthe electromagnetic spectrum\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe magnitude of energy that an object reflects or emits across a range of wavelengths is called its spectral response pattern/spectral signatures. It can be used to identify objects.\n\n\nResolution\nIn remote sensing we refer to three types of resolution: spatial, spectral and temporal. Spatial Resolution refers to the size of the smallest feature that can be detected by a satellite sensor or displayed in a satellite image. It is usually presented as a single value representing the length of one side of a square. Remote Sensing » Spatial Analysis\n\n\n\nWe can use:\nCopernicus Open Accesss Hub to download RS images collected by Sentinel Series Satellite.\n\n\n\n\n\n\nNote\n\n\n\nFor Sentinel Series Satellite Overview\nPS. Very few of the links for additional information are in Chinese, as this makes it easier for me to understand.\n\n\nNote when selecting products, be careful about：\n\nSatellite transit period range (e.g. 2020/01/01 - 2020/03/07)\nData type (Sentinel1/2/3) Where Sentinel2 data can be downloaded in types S1C (unatmospherically corrected) and S2A (atmospherically corrected).\nCloud cover to extent [0 TO 5]\n\nWe can also use:\nLandsat data to download RS image from Landsat Series.\n\n\n\nCopernicus Open Accesss Hub\nGRANULE > sensor number > IMG_DATA > R10\nThen you can see the information about the bands.Including its resolution, central wavelength and description about which wave band it belongs to ( Blue/Green/Ted/VNIR/…)\nAnd here is a website which provides basic information from the characteristics of satellite to its products. For example ,this is for Landsat 5 (TM)\n\n\n\nQGIS\n\n\n\n\n\n\nTip\n\n\n\nTo process Sentinel data with QGIS, we can install Semi-Automatic Classification Plugin (SCP) version 7.\n\n\nWe can open monochrome images in different bands. We can also open the TCI image, which is a True Colour Image of B02 (Blue), B03 (Green), and B04 (Red) Bands.But the TCI has a range of pixel values from 0-255, it is not visually clear. We can get a more realistic level of colour by using BOA data.\n\n\n\n\n\n\nThe BOA bands\n\n\n\nIn the Sentinel 2 images available for download, you will find Level 1C data and Level 2A data. 1C data are orthorectified and geometrically refined atmospheric apparent reflectance products that are not atmospherically corrected. 2A images are orthorectified bottom-of-atmosphere (BOA) reflectance corrected images. What does this mean? Unlike Class 1C, Class 2A images correspond to atmospherically corrected images and provide reflectance data that are closer to the real thing (and therefore have a more realistic colour level). It is possible to distinguish between them visually (below), as a Class 2A image is sharper, has higher brightness and contrast, and does not show the whitish texture produced by atmospheric influences.\n\n\nIn addition, using the merge tool to merge single-band images can also obtain multi-band images. Then realize True Color Image by selecting the red, green and blue bands.\nSNAP\nSNAP is an application developed by the European Space Agency for processing Sentinel satellite data. We actually can open .zip file in SNAP which is very convenient. And if unzip the file and open each .tiff file separately, some important information will lose like the value of wavelength.This video can be help on how to open Sentinel data in SNAP.\nFollowing are some analysis we can do through the software:\n\nColour composites\n\nWe can achieve different band combinations by selecting different remote sensing image bands in RGB channels\n\n\n\n\n\n\nTip\n\n\n\nBe careful that different RS data may have different meanings for the band with the same No.\n\n\nFor Sentinel 2 data：\n\nSentinel 2 Bands and Combinations\n\n\n\n\n\n\n\nTypes\nRGB channels\ndescription\n\n\n\n\nNatural Color\nB4, B3, B2\ndisplay imagery the same way our eyes see the world\n\n\nColor Infrared\nB8, B4, B3\nemphasize healthy and unhealthy vegetation. denser vegetation is red,but urban areas are white\n\n\nAgriculture\nB11,B8,B2\nused to monitor the health of crops,highlighting dense vegetation that appears as dark green\n\n\nGeology\nB12, B11, B2\nfor finding geological features. This includes faults, lithology, and geological formations\n\n\n\nFor Landsat 5 TM data：\n\nLandsat 5 TM Bands and Combinations\n\n\n\n\n\n\n\nTypes\nRGB channels\ndescription\n\n\n\n\nNatural Color\nB3, B2, B1\ndisplay imagery the same way our eyes see the world\n\n\nStandard false color\nB4, B3, B2\nemphasize healthy and unhealthy vegetation. denser vegetation is red,but urban areas are white\n\n\nsimulated true color\nB7,B4,B3\nIdentify residential land and water bodies\n\n\n…\n…\n…\n\n\n\n\n\n\nColor Infrared\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn addition to band combination, we can also use some band calculations to obtain images that can highlight targets.For example, Vegetation Index (B8-B4)/(B8+B4), Moisture Index (B8A-B11)/(B8A+B11)\n\n\n\nImage statistics\n\nHistogram\nA graph that describes the relationship between each gray level in an image and its frequency of occurrence. Histograms for each band of the image can be viewed in SNAP. If we change the mapping range of image pixels in 0-255, the image will be changed visually. We generally omit 1% at the lower end and 4% at the upper end for mapping. In addition, image contrast can be improved by adjusting the histogram morphology.\n\n\n\n\n\n\nNote\n\n\n\nMean: reflects the overall brightness of the image\nVariance: refers to the variance of the brightness value of each band, reflecting the volume of the information\n\n\nScatterplot\nThe scatterplot is used to analyze the spatial distribution relationship between two bands. For example, draw a scatter diagram of red (vegetation absorbs) and Near-infrared (NIR, that vegetation strongly reflects). Since higher NIR values and lower red values indicate dense vegetation, while lower values of both are usually bare soil. Therefore, the image can roughly analyze the distribution characteristics of different ground objects.\n\n\n\nScatter Plot\n\n\n\nMasking and resampling\n\nRemote sensing images can be cropped using vector data (Raster > Masks > Land/Sea mask) to leave areas of interest. Note that SNAP can only read ESRI shapefiles. In addition, before cropping, we need to unify the resolution of the band, you can use the resampling tool (Raster > Geometric > Resampling)\n\nComparison of spectral signatures\n\nWe can extract the spectral signatures of different land cover classes using the Semi-automatic Classification plugin in QGIS 3.16. Here is a tutorial video\n\n\n\nSpectral Signatures in QGIS\n\n\nAlso, it is easy to do it in SNAP. Basically you just need to use Optical->Spectrum View, and make sure that your image has band information. Besides, you can use Pin tool to make sample, and compare the curve of different objects. Here is another tutorial video about Pins and Spectrum Tool in SNAP\n\n\n\nSpectrum View in SNAP"
  },
  {
    "objectID": "WEEK1.html#application",
    "href": "WEEK1.html#application",
    "title": "1  WEEK1",
    "section": "1.2 Application",
    "text": "1.2 Application\nThe main application of Remote Sensing Image is about recognizing geographical entities, which is well used in agriculture, forestry, geology, marine, meteorology, hydrology, military, environmental protection and other fields.Here, I will give same simple example of how we use remote sensing image in agriculture scenarios.\n\n\n\nUsing RS to predict corn harvest time, Source: Janoušek et al. 2021\n\n\nCrop remote sensing production estimation is based on the collection of different spectral signatures of various crops in different growth stages according to biological principles, through the surface information recorded by the sensor on the platform, to identify crop types, monitor crop growth, and predict crop yield before crop harvest a range of methods. This technology can dynamically monitor the growth process of crops, measure the planting area, estimate the output per unit area and estimate the total output.\nTraditionally, optical and infrared sensors will be used to detect the irrigated areas, for example, Thenkabail et al. (2005) developed a comprehensive algorithm based on timeseries of MODIS spectral bands (2, 3, 5, 6, and 7) data to detect irrigation and rainfed classes along with crop onset, peak, and senescence (aging of the plant).\nAnd for crop yield assessment, using remote sensing of plant photosynthetic activity is a good way as photosynthetic activity influences biomass production. Both passive and active microwave remote sensing can help us to do it. There are some indicators or models like the Vegetation Optical Depth (VOD) (Guan et al. 2016). Besides, regression relationship with indices such as NDVI, RVI, and VCI are used to estimate crop yield.\n\n\n\nSource: Tolomio et al. 2020\n\n\nIn addition, remote sensing technology can also be useful when detecting agricultural pests and diseases. Indices such as Disease Water Stress Index (DWSI) , Disease Index (DI) and Yellow Rust Index (YRI) for detecting wheat yellow rust, Aphid Index (AI), and Leafhopper Index (LHI) , among others are proposed for pest and disease detection (Karthikeyan et al. 2020)."
  },
  {
    "objectID": "WEEK1.html#reflection",
    "href": "WEEK1.html#reflection",
    "title": "1  WEEK1",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\n\nFor remote sensing image data, the products generated by different sensors are very different, such as the information of bands and the structure of downloaded files. This requires us to have a good understanding of these video products before processing the data, otherwise you don’t even know how to open them in the software.\nAs for software, these software are not very friendly to novices, such as densely packed menu bars and tools that don’t know where they are. Fortunately, YouTube has many tutorials to help you operate.\nThe subject of remote sensing involves some physical knowledge, such as spectrum and radiation, which is sometimes a bit difficult to understand. The remote sensing of the visible light part can be interpreted through eyes which is helpful to understand. Others, such as microwave remote sensing, are even more difficult to understand.\nHowever, when I was studying agricultural remote sensing, I felt the power of this technology. When the study area is large, remote sensing is a good choice to monitor dynamic changes. Moreover, it can digitize the characteristics of geographic entities, such as the reflectivity of the electromagnetic spectrum with different wavelengths. This digitization provides opportunities for further data analysis (and the scale of the data is easily consistent, this is great). Therefore, a large number of ML and AI technologies are applied to remote sensing image processing. This brings greater promise to the application of remote sensing."
  },
  {
    "objectID": "WEEK1.html#reference",
    "href": "WEEK1.html#reference",
    "title": "1  WEEK1",
    "section": "1.4 Reference",
    "text": "1.4 Reference\n        Janoušek, Jiří, et al. “Using UAV-based photogrammetry to obtain correlation between the vegetation indices and chemical analysis of agricultural crops.” Remote Sensing 13.10 (2021): 1878.\n        Grant, J. P., et al. “Comparison of SMOS and AMSR-E vegetation optical depth to four MODIS-based vegetation indices.” Remote Sensing of Environment 172 (2016): 87-100.\n        Thenkabail, Prasad S., Mitchell Schull, and Hugh Turral. “Ganges and Indus river basin land use/land cover (LULC) and irrigated area mapping using continuous streams of MODIS data.” Remote Sensing of Environment 95.3 (2005): 317-341.\n        Karthikeyan, L., Ila Chawla, and Ashok K. Mishra. “A review of remote sensing applications in agriculture for food security: Crop growth and yield, irrigation, and crop losses.” Journal of Hydrology 586 (2020): 124905.\n        Huang, Jianxi, et al. “Assimilation of remote sensing into crop growth models: Current status and perspectives.” Agricultural and Forest Meteorology 276 (2019): 107609.\n        Tolomio, Massimo, and Raffaele Casa. “Dynamic crop models and remote sensing irrigation decision support systems: A review of water stress concepts for improved estimation of water requirements.” Remote Sensing 12.23 (2020): 3945.\nAnd some other website may be useful: Learn GIS and Geography"
  },
  {
    "objectID": "WEEK2.html",
    "href": "WEEK2.html",
    "title": "2  WEEK2",
    "section": "",
    "text": "1 2 3 4 5"
  },
  {
    "objectID": "WEEK3.html",
    "href": "WEEK3.html",
    "title": "3  WEEK3",
    "section": "",
    "text": "sensor and image\n\n\n\n\nHere shows a basic flow of how remote sensing is used in practice, it includes a few steps from data collection. As there are many factors influencing the RS image acquisition like, sensor characteristics, weather (particularly cloud cover), acquisition method (satellite or airborne) and others, remotely sensed images can contain flaws within them. Therefore, to improve image quality as a basis for further analysis, the most common step is carrying out corrections.\n\n\n\n\nflowchart LR\n  A(Acquisition) --> B(Pre-processing)\n  B --> C(Image Analysis)\n  C --> D(Products)\n  D --> E(Application)\n\n\n\n\n\n\n\n\n\nTo talking about this processing chain we can introduce the product levels. And we will summary some main points about each correction.\n\n\n\ncorrection and product levels\n\n\nThese are some website helpful to understand each process,\nThe Importance of Radiometric Calibration\nGeometric Corrections in Remote Sensed Image\nWhat is Atmospheric Correction in Remote Sensing?\nand following are some summaries.\nRadiometric Calibration\nRadiometric Calibration refers to the ability to convert the digital numbers recorded by satellite imaging systems into to spectral radiance, reflectance, or brightness temperatures.\nThe purpose is to eliminate the error of the sensor itself and determine the accurate radiation value at the entrance of the sensor.\nMethods include laboratory calibration, on-board/satellite calibration, and site calibration. Note that different sensors have different radiation calibration formulas.\n\n\n\n\n\n\nHow to tell if your imagery is lacking a radiometric workflow:\n\n\n\nWithout radiometric calibration, you may see the following effects:\n\nUnderexposed images, especially surrounding bright objects on the landscape\nIrregular coloration\nIndex values, such as NDVI, that appear to change dramatically and unexpectedly near roads or buildings\nExtreme banding or patchiness in your mosaic\n\n\n\nGeometric Correction\n\nCauses\n\nCurvature of the earth\n\nTopography of the terrain\n\nEarth rotation\n\nView angle (off-nadir)\n\n…\n\nCorrection Steps\n\nAcquisition of ground control points (GCP) : Use Ground Control Points (GCP) as a reference point for correct location data\nBuilding a Sensor Model: formulate the relation between the current and the reference location data\nCoordinate Transformation: Once a suitable sensor model is defined it can be applied to the image raw data to calculate the corrected coordinates of the pixel locations\nResampling of Pixel Values: The coordinate transformations shifts and rotates the pixel locations. These new locations do not fit into a regular grid of the raster files. Therefore, the pixel values need to be resampled to assign each grid cell one raster values.\n\n\n\n\n\n\nWhat is a ground control point (GCP)?\n\n\n\nGround Control Points (GCPs) are defined as points on the surface of the earth of known location used to geo-reference Landsat Level-1 data. GCPs are updated as needed to continually improve Landsat data. GCPs can be downloaded and used as reference data.\n\n\n\nExamining the Accuracy\n\nRoot mean squared error can be used to measure the difference between locations that are known and locations that have been interpolated or digitized or resampled. \\[\\begin{equation}\nRMS_{error}=\\sqrt{(X^{'}-X_{orig}+Y^{'}-Y_{orig})^2}\n\\end{equation}\\]\nAtmospheric Correction\nremoves the scattering and absorption effects from the atmosphere to obtain the surface reflectance characterizing (surface properties)\n\nCauses\n\nAtmospheric scattering\nAtmospheric reflections\nTopographic attenuation\n\n…\n\nCorrection Methods\n\nMethods can be divided into 2 types according to the results of the correction:\n\nAtmospheric Correction\n\n\n\n\n\n\nType\nNotes\n\n\n\n\nRelative atmospheric correction method\nfor example the dark object subtraction (DOS), histogram adjustment and psuedo-invariant features (PIFs)\n\n\nAbsolute atmospheric correction method\nthere are some transfer models or codes such as MODTRAN 4+ and 6S can be used to deal with scattering and absorption, this is called model-based atmospheric correction methods. We can also use the empirical line correction by take measurements in site using a field spectrometer.\n\n\n\nOrthorectification Correction\northorectification is the process of improving the horizontal accuracy of imagery.\n\nCauses\n\nCamera and sensor orientation\nTopographic relief displacement\nEarth curvature\n\n…\n\nCorrection Methods\n\nWe can use some orthorectification algorithms to deal with the distortion, also, softerwares like QGSIS and SAGA GIS can be helpful.\n\n\n\n\n\n\nOrthorectification vs Georectification\n\n\n\nGeorectify take an image that has not been adjusted to be in a known coordinate system, and put it into a known coordinate system.\nOrthorectify take an image in its original geometry and very accurately adjust it so that it is in a known coordinate system, with distortions due to topographic variation corrected.\n\n\n\n\n\n\nImage enhancement algorithms are applied to remotely sensed data to improve the appearance of an image for human visual analysis or occasionally for subsequent machine analysis. There is no such thing as the ideal or best image enhancement because the results are ultimately evaluated by human.\n\nImage enhancement is attempted after the image is corrected for geometric and radiometric distortion. here we will summarize some of the most popular enhancements techniques.\nContrast Enhancement\n\nWhy it is needed\n\nMost of the satellite images lack adequate contrast with nearly uniform tones of gray.This results in difficulty in interpret the image. Reasons for low contrast of image can be various. For example, the objects may have a nearly uniform electromagnetic response at the wavelength band of energy, scattering of electromagnetic energy by the atmosphere can reduce the contrast of a scene…\n\nPrinciple\n\nContrast enhancement techniquest (also referred to as contrast stretching) expand the range of brightness values in an image so that the image can be efficiently displayed in a manner desired by the analyst.\n\nMethods/Examples\n\nContrast enhancement techniques can be divided into 2 categories.\n\nContrast Enhancement\n\n\nType\nNotes\n\n\n\n\nLinear Contrast Enhancement\nincluding Minimum-Maximum Contrast Stretching, Percentage Linear and Standard Deviation Stretching,Piecewise Linear Contrast Stretching. They differ in the Stretching Function. Like the picture below shows how Minimum-Maximum Contrast Stretching works.\n\n\nNonlinear contrast enhancement stretching\none applied of the most useful enhancements is histogram equalization. In this technique, histogram of the original image is redistributed to produce a uniform population density. The whole process can be a little complex, which can be found here at Page 8\n\n\n\n\n\n\nLinear contrast stretch versus histogram equalization\n\n\nBand Ratioing\n\nWhy it is needed\n\nSometime differences in brightness value from identical surface materials are caused by topographic slope and aspect, shadow, or seasonal change in sun illumination angle and intensity. These conditions may hamper ability of interpreter or classification algorithm to identify correctly surface materials or land use in a remotely sensed image. We can use Band Ratios to enhance the spectral differences between bands and to reduce the effects of topography.\n\nPrinciple\n\nBasically, Band Ratioing is about doing band algebra, dividing one spectral band by another produces an image that provides relative band intensities.The ratio is the numerator divided by the denominator\n\nMethods/Examples\n\nThere are many defined indexes show us the way to do band ratioing, such as The Normalised Difference Vegetation Index (NDVI) which is based on the fact that healthy and green vegetation reflects more in the NIR but absorbs in the Red wavelength. \\[\\begin{equation}\nNDVI=\\frac{NIR-Red}{NIR+Red}\n\\end{equation}\\]\nThere are many other ratios, all of which are detailed on the Index Database\n\n\n\nThe sunlight causes the different value of land cover, but the ratio is constant\n\n\nSpatial filtering\n\nA characteristic of remotely sensed image is a parameter called spatial frequency; defined as the number of changes in brightness value per unit distance for any particular part of an image.\n\n\nWhy it is needed\n\nSpatial filters serve a variety of purposes, such as detecting edges along a specific direction, contouring patterns, reducing noise, and detail outlining or smoothing. Filters smooth, sharpen, transform, and remove noise from an image so that you can extract the information you need.\n\nPrinciple\n\nSpatial filtering applies the idea of convolution, which is an algorithm that consists of recalculating the value of a pixel based on its own pixel value and the pixel values of its neighbors weighted by the coefficients of a convolution kernel. Basically, to acheive Spatial Filtering we firstly decide a window, that is the size of the convolution kernel(3 × 3, 5 × 5…can be either square or rectangular). And according to different types of convolutions:gradient, Laplacian, smoothing, and Gaussian, different convolution kernel contents or the weight will be used to calculated the new value of the central kernel pixel.\nFrom the perspective of mathematical calculation, it is to construct a matrix, multiply the elements of the matrix by the value of the corresponding pixel, and then add up the products as the new value of the central pixel. Like the picture blew illustrates.\n\n\n\nExample of Spatial filtering\n\n\n\nMethods/Examples\n\nSpatial filters fall into two categories:\n\nSpatial filtering\n\n\n\n\n\n\nType\nNotes\n\n\n\n\nHighpass filters\nemphasize significant variations of the light intensity usually found at the boundary of objects. Highpass frequency filters help isolate abruptly varying patterns that correspond to sharp edges, details, and noise.\n\n\nLowpass filters\nattenuate variations of the light intensity. Lowpass frequency filters help emphasize gradually varying patterns such as objects and the background. They have the tendency to smooth images by eliminating details and blurring edges.\n\n\n\nSpecific tye of Spatial Filter and its application can be found in here\nThere are also other way to process the image and gain information we needed, like Texture, Data fusion and PCA, here I am trying to use R language to achieved them. In addition, paying special attention on them in the application part.\n\n\n\nHere I will use R to achieve some enhancement methods. Firstly we use pull() to load our raster layers into a stack, then we can easily pick out one of the band.\n# List your raster files excluding band 8 using the patter argument\nm1<-dir_info(here::here(\"LC09_L2SP_203023_20220326_20220328_02_T1\")) %>%\n  dplyr::filter(str_detect(path, \"[1B23456790].TIF\")) %>%\n  dplyr::select(path) %>%\n  pull() %>%\n  as.character() %>%\n  # Load our raster layers into a stack\n  terra::rast()\nreferring the equation given above, the NDVI can be calculated quickly,\nm1_NDVI <- (m1$LC09_L2SP_203023_20220326_20220328_02_T1_SR_B5 - m1$LC09_L2SP_203023_20220326_20220328_02_T1_SR_B4 ) / (m1$LC09_L2SP_203023_20220326_20220328_02_T1_SR_B5 + m1$LC09_L2SP_203023_20220326_20220328_02_T1_SR_B4)\n\n\n\nFigure of NDVI >0.2 the high value stands for the healty plant\n\n\nand the glcm package can help us to do the Texture Analysis.\nglcm(x, n_grey = 32, window = c(3, 3), shift = c(1, 1), statistics = c(\"mean\", \"variance\", \"homogeneity\", \"contrast\", \"dissimilarity\", \"entropy\", \"second_moment\", \"correlation\"), min_x=NULL, max_x=NULL, na_opt=\"any\", na_val=NA, scale_factor=1, asinteger=FALSE)\n#statistics means a list of GLCM texture measures to calculate\nif we usd the homogeneity measure the results is shown blow\n\n\n\nthe homogeneity of the image\n\n\n\n\n\n\n\n\nHomogeneity texture measure\n\n\n\nHomogeneity was a measure of the similarity of pixel values in the neighborhood defined by the processing window, ranging from 0.0 (completely dissimilar) to 1.0 (all cells having equivalent values) (Lane CR et al. 2014)\n\n\nthe video Introduction to textural classification in QGIS 3.10 shows basic concept of Texture Analysis as well as how to do it in QGIS.\nFor PCA, rasterPCA can be helpful to reduce the dimensionality of our data."
  },
  {
    "objectID": "WEEK3.html#application",
    "href": "WEEK3.html#application",
    "title": "3  WEEK3",
    "section": "3.2 Application",
    "text": "3.2 Application\nTexture Analysis in Classification\nIn remote sensing image, texture means the spatial variation of the pixels in an small area. So, it is different from what the image visually looks like. Coburn, C. A., & Roberts, A. C. (2004) pointed out that of all the spatial information that can be extracted from remotely sensed data, texture may be the most useful for segmenting images. Therefore, Texture Analysis can provide information that the visual interpretation cannot. when we are trying to distinguish the geographic entities or the land cover. Here is a example about how texture analysis can be combined with Random Forest technique to accurately differentiate land covers of urban vegetated areas.\nFeng Q et al. (2015) achieved urban vegetation mapping by using UAV Remote Sensing data. Its whole processes include 3 parts:data acquisition and preprocessing;image classification; accuracy analysis. The Random Forest and texture analysis were introduced in the second part.\nFor the texture analysis, six least correlated texture measures according to Szantoi Z et al. (2013) were used in the study(mean (MEA), standard deviation (STD), homogeneity (HOM), dissimilarity (DIS), entropy (ENT) and angular second moment (ASM)). Besides, only Green band was used for the texture statistics and they were second-order (co-occurrence) textures.\n\n\n\nThe workflow of the study\n\n\n\n\n\n\n\n\nFirst-Order (Occurrence) and Second-Order (Co-Occurrence)\n\n\n\n\nFirst-order metrics operate on the counts, or occurrences, of the different digital number (DN) values within a kernel. Separate images with different spatial arrangements of the same pixels within a single kernel will yield the same first-order texture value in both kernels.\nSecond-order (also called co-occurrence) metrics analyze the relationship between pixel pairs. They use a co-occurrence matrix to calculate texture values. This matrix is a function of both the angular relationship and distance between two neighboring pixels. It shows the number of occurrences of the relationship between a pixel and its specified neighbor.\n\nfind more at Texture Metrics Background\n\n\nHere, we skip the steps the author calculated the textures, just remember that 9 texture window sizes were chosen and texture features derived at different window sizes were added separately as additional ancillary bands to the RGB images for further classification.\nWe will not talk about Random Forest too much here, it is a supervised classification and the UAV orthophotos and ancillary texture features were inputs.\nHere is the figure in the paper which shows 2 original RGB images. According to field survey and the aim of the project, three vegetated land covers were chosen as follows: grass, trees and shrubs\n\n\n\nThe original RGB images\n\n\nAnd here is the results of classification. For the picture below, the GRB-only means only used GRB bands as inputs while the RGB+GT31 means the texture metrics with the moving windows 31 × 31 (GT31) included. So, this figure depicts the differences before and after the inclusion of texture features. We can easily see that both Image-A and Image-B show some “salt and pepper” effects.And the inclusion of texture improves the classification accuracy from a visual point of view. This is because the inclusion of texture increases the between-class separability and tends to remove small isolated groups of pixels.\n\n\n\nThe classification results.(a) Image-A RGB-only; (b) Image-A RGB+GT31; (c) Image-B RGB-only; (d) Image-B RGB+GT31\n\n\nIn addition, if looking at the details, we can see that some part of the area with tree, grass and bare soil come together, some pixels are misclassified in RGB-only classification. This may due to tree, dead shrubs and bare soil share similar colors. But these misclassifications can be improved after the inclusion of six texture features at window size 31.\nIn conclusion, this study manifests that the including of texture features can increase the classification accuracy when differentiating land covers of urban vegetated areas. It also shows us the impact of texture window size on classification accuracy, which turns out has an inverted U relationship.\nHowever, personally speaking, I think there are still some points that the author hasn’t provided enough information. For example, why the Second-order metrics was chosen here instead of the First-Order one? Will there be any difference of the classification results if we use the latter? In addition, I am not very sure about why the experiment used 6 texture measures. Does the contribution of these texture measures to the classification make a difference?"
  },
  {
    "objectID": "WEEK3.html#reflection",
    "href": "WEEK3.html#reflection",
    "title": "3  WEEK3",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\n\nAs can be seen from the flow chart and product levels diagram in 3.1.1, there is still a long data processing process between the creation of a remote sensing image by the sensor and the use of it to solve practical problems. This shows that remote sensing is not equal to taking a picture of the ground in a very distant place. The study of correction has deepened my understanding of electromagnetic radiation and the influence of different factors on radiation. From the perspective of problem solving, it allows us to clearly understand what happens to different radiations from generation to acceptance by the sensor. For example, Radiometric Calibration means that the sensor itself may have errors, Geometric Correction shows the influence of topography of the terrain on the image, and Atmospheric Correction shows the weakening and scattering by the atmosphere in the entire electromagnetic wave radiation process.\nImages are a way of expressing information, and remote sensing images are no exception. For ordinary images, we can use the components of the image space for interpretation, that is, interpretation based on vision. For example, vegetation is green and water is blue. Therefore, on the one hand, the contrast enhancement of remote sensing images can expand the difference between pixel value of objects, which is very beneficial to image interpretation. In many cases, we will adjust the histogram to highlight the features we want to observe. In addition, the segmented gray-scale stretching can target the dark and bright parts of the image to suppress low-frequency parts or enhance high-frequency parts as you decide.\nHowever, the land use distribution of real scenes is often complex, and when the surface area of the image cover is large, it is difficult to distinguish geographical entities only by the difference in hue. The misclassified situation will happen especially in complex terrain (such as mountainous terrain). At this time, the importance of spatial structure information is highlighted. Image texture is useful for the separation of different categories of regions, which is related to the visual perception of roughness or smoothness of image features. A large number of studies have shown that adding texture information as a band to image classification is conducive to improving the accuracy of the classification structure. This reflects the advantage of image enhancement processing, that is, the ability to obtain spatial information.\nFurthermore, in the actual process, some difficulties may be encountered in image enhancement processing. For example, when the image quality is not good, some noise may also be enhanced. So this is like a double-edged sword. When using an enhancement algorithm, you should consider distinguishing between useful information and noise in the image. Ali et al. (2001) pointed out that for noisy remote sensing images, different edge detection operators detect different edge maps. Some of these algorithms do not perform well. This means that we need to be careful when doing image enhancement, otherwise we may go the wrong way。"
  },
  {
    "objectID": "WEEK3.html#reference",
    "href": "WEEK3.html#reference",
    "title": "3  WEEK3",
    "section": "3.4 Reference",
    "text": "3.4 Reference\n        Feng, Q., Liu, J., & Gong, J. (2015). UAV remote sensing for urban vegetation mapping using random forest and texture analysis. Remote sensing, 7(1), 1074-1094.\n        Coburn, C. A., & Roberts, A. C. (2004). A multiscale texture analysis procedure for improved forest stand classification. International journal of remote sensing, 25(20), 4287-4308.\n        Szantoi, Z., Escobedo, F., Abd-Elrahman, A., Smith, S., & Pearlstine, L. (2013). Analyzing fine-scale wetland composition using high resolution imagery and texture features. International Journal of Applied Earth Observation and Geoinformation, 23, 204-212.\n        Lane, C. R., Liu, H., Autrey, B. C., Anenkhonov, O. A., Chepinoga, V. V., & Wu, Q. (2014). Improved wetland classification using eight-band high resolution satellite imagery and a hybrid approach. Remote sensing, 6(12), 12187-12216.\n        Ali, M., & Clausi, D. (2001). Using the Canny edge detector for feature extraction and enhancement of remote sensing images. IEEE International Symposium on Geoscience and Remote Sensing (IGARSS), 5, 2298-2300."
  },
  {
    "objectID": "WEEK4.html",
    "href": "WEEK4.html",
    "title": "4  WEEK4",
    "section": "",
    "text": "The learning diary of this week is about: Policies in cities, and how we can use RS technology to achieve the goals/ideas behind the policy. And here we give an example in Pau da Lima, City of Salvador, Brazil."
  },
  {
    "objectID": "WEEK4.html#policy",
    "href": "WEEK4.html#policy",
    "title": "4  WEEK4",
    "section": "4.1 Policy",
    "text": "4.1 Policy\nIn this case study, the NEW!! World Cities Report 2022 is the intact referring policy. And the Chapter 10: Building Resilience for Sustainable Urban Futures (Page.334) is the section particularly been focused on. There are some of the Policy points mentioned in this part:\n\n\nInvesting in key urban infrastructure must be a prerequisite for building sustainable and resilient urban futures.\nPolicymakers must match urban risk assessments with appropriate solutions\nVisioning and implementation of urban resilience plans must prioritize the poorest and most vulnerable communities.\nBuilding urban resilience will not succeed without public participation.\n\n\nSo the infrastructure, urban risk, communities in disadvantage are highlighted here. It reminds us the vulnerability of slums and informal settlements due to the poor infrastructure is far from resilience.\nFollowing the report, in 10.4. Environmental Resilience, it talks about raising awareness of different local urban risks and identification of feasible disaster prevention and preparedness. In which the discussion of flood is involved:\n\n…focused principally on flood risk, with imaginative awareness raising strategies and mitigation strategies including a shift from traditional hard engineering solutions towards naturebased solutions and ecosystem services such as restoring and expanding riverine vegetation and floodplains…\n\nAnd next, the report emphasizes on building resilience in slums and informal settlements, this is because people and areas there are facing particular vulnerabilities and high risks.\nTherefore, referring the policy, this case study tried to pay attention to building resilience in slums and informal settlements. Focused principally on flood risk. The policy suggests that we could achieve the development of resilient cities with:\n\n…imaginative awareness-raising strategies and mitigation strategies including a shift from traditional hard engineering solutions towards nature-based solutions and ecosystem services…\n\nThat is quite abstract. However, as RS technology is closely related to the nature, and can be used in both hard engineering and software system, I think we can achieve it."
  },
  {
    "objectID": "WEEK4.html#study-area-and-the-problem-desription",
    "href": "WEEK4.html#study-area-and-the-problem-desription",
    "title": "4  WEEK4",
    "section": "4.2 Study Area and the Problem Desription",
    "text": "4.2 Study Area and the Problem Desription\n(Slum community) Pau da Lima, City of Salvador, Brazil\n\n\n\nPau da Lima\n\n\nIn total, 67% of the population of Salvador and 37% of the urban population in Brazil reside in slum communities with equal or greater levels of poverty as that found in Pau da Lima. And a tropical rainforest country, the average monthly precipitation is more than 60 mm, which shows that the area has a large amount of precipitation all year round. However, the area’s drainage system is unstable and has open sewers, so overflows often occur during periods of heavy rain, and the valley floor is prone to flooding. Sanitary conditions in the area are generally poor, and residents are often exposed to garbage and sewers. Besides, the buildings in slum areas are informal, so their structure is often unstable which will cause danger in storm rain periods."
  },
  {
    "objectID": "WEEK4.html#proposed-measures",
    "href": "WEEK4.html#proposed-measures",
    "title": "4  WEEK4",
    "section": "4.3 Proposed Measures",
    "text": "4.3 Proposed Measures\n\n4.3.1 Assessment\n\nTargets and Methods for Flood Risk Assessment\n\n\n\n\n\n\n\n\nName\nNotes\nAims\nData and Methods\n\n\n\n\nLand-use and Land-cover recognition\nThe complex distribution of buildings in slum areas and the lack of a clear building plan make it difficult to map the areas. Furthermore, the Pau da Lim slum is in a valley area and is heavily vegetated, which makes it more difficult to classify land use\nIdentify the distribution of settlements, reservoirs, sewers, etc. in slums\nUsing multispectral or hyperspectral imagery, combined with image recognition and machine learning algorithm techniques; Using SAR for texture analysis to identify residential buildings\n\n\nTerrain detection\nFor houses in low-lying areas and in flood-prone areas, the risk factor is higher in flooding\nAssessment of the hazard index at the terrain level\nUsing LIDAR technology to obtain topographic elevations and generate DEM data\n\n\nPhysical safety of buildings measurement\nMany buildings in slum areas are not built to code and have unstable roofs, which can easily fall off during heavy rainfall and cause injuries\nDetection of dangerous buildings\nUsing VHR images to extract physical, geometric features of buildings\n\n\nEnvironmental Assessment\nPoor hygiene around the house, such as sewage running through or open piles of rubbish, can cause serious health problems when flooding\nAnalysis of the environmental health of the surroundings of the residence\nMonitoring of water quality parameters (e.g. water temperature, cyanobacteria concentration, chlorophyll concentration, etc.) in water bodies using meteorological satellites for effluent identification\n\n\n\n\n\n4.3.2 Reconstruction\nReferring to the results of the assessment, dangerous houses are rebuilt and houses in flood-prone areas unsuitable for building houses are relocated. In addition, the layout of sewerage pipes can be adapted to the land use situation, etc.\n\n\n4.3.3 Monitoring\n\nTargets and Methods for Monitoring Flood\n\n\n\n\n\n\n\nName\nAims\nData and Methods\n\n\n\n\nPrecipitation monitoring\nReal-time monitoring of the relationship between soil moisture content and rainfall in the area to predict the occurrence of flooding\nUsing the Advanced SCATterometer (ASCAT) on MetOp for soil moisture observations, combined with the SM2RAIN-ASCAT global-scale rainfall product dataset for flooding predictions\n\n\nReal-time detection of flooding disasters\nIdentification of the area currently affected by flooding\nUsing Landsat5TM data and the Normalised Difference Water Index (NDWI) for the extraction of water bodies. If the weather is bad, SAR can also be used too\n\n\nDisaster assessment\n\nComparing remote sensing images before and after flooding\n\n\nLand use change analysis\nAnalysis of the expansion of the study area and adjustment of the drainage system to the changes in land use\nLike the data and methods of Land-use and Land-cover recognition"
  },
  {
    "objectID": "WEEK4.html#summary",
    "href": "WEEK4.html#summary",
    "title": "4  WEEK4",
    "section": "4.4 Summary",
    "text": "4.4 Summary\nFrom understanding, to prevention, and to handling"
  },
  {
    "objectID": "WEEK4.html#reference",
    "href": "WEEK4.html#reference",
    "title": "4  WEEK4",
    "section": "4.5 Reference",
    "text": "4.5 Reference\n        Munawar, H. S., Hammad, A. W., & Waller, S. T. (2022). Remote sensing methods for flood prediction: A review. Sensors, 22(3), 960.\n        Kuffer, M., Pfeffer, K., & Sliuzas, R. (2016). Slums from space—15 years of slum mapping using remote sensing. Remote Sensing, 8(6), 455.\n        Reis, R. B., Ribeiro, G. S., Felzemburgh, R. D., Santana, F. S., Mohr, S., Melendez, A. X., ... & Ko, A. I. (2008). Impact of environment and social gradient on Leptospira infection in urban slums. PLoS neglected tropical diseases, 2(4), e228.\n        Klemas, V. (2015). Remote sensing of floods and flood-prone areas: An overview. Journal of Coastal Research, 31(4), 1005-1013."
  },
  {
    "objectID": "WEEK4.html#summary-and-reflection",
    "href": "WEEK4.html#summary-and-reflection",
    "title": "4  WEEK4",
    "section": "4.4 Summary and Reflection",
    "text": "4.4 Summary and Reflection\n\nFrom understanding, preventing, and to handling the flood disaster in slum areas, this case study shows the possibility that RS technology can be helpful when constructing a resilient city.\nFor the whole process, this study is about measuring the risk level of each area. Before the flood, planning to reconstruct the infrastructure like the drainage system, remove settlements in dangerous areas, etc, so we can defend against the flood better and minimize the damage it causes. And when the flood is coming, we hope there is a system that can predict its accuracy, giving people adequate reaction time. This would be with the help of a monitoring system. In addition, during and after the flood, the system can provide information on the impact of disasters in different regions. This is especially helpful for rescue and re-planning in the future.\nThe advantage of RS is its real-time response to environmental changes. This is crucial when dealing with issue associated with nature. Besides, as a technology based on data, it can well applied in different system, playing a role of information provider.\nHowever, there are also certain challenges in applying RS in this case. The most nonnegligible and realistic point is the investment of money and professionals. Although urban issues and slum issues both involve geographic space and the social activities of people, the former are often considered worthy of costly long-term research. For the latter, a large number of existing slum rescue projects are based on “demolition” and “reconstruction”, so whether it is reasonable to invest a lot of manpower and money to build a RS system for an object that only exists for a limited time needs to be considered by the local government.\nOf course, one solution is to build a flexible system that can be used in more than one place, no matter in downtown area or in slum. And this will face another major challenge, that is, the huge differences of areas.This includes, but is not limited to climate, topography, architectural structure and the stage of development of the city. Therefore, we need a large number of RS-based practices, so that we can provide references this kind of projects in the future.\nAnd last, for the policy. I think most of the policies show a real ideal and satisfactory planning for the future city while some of them are very vague. if we can catch the essence of the policy, we may come up with measures that can achieve the goal. For example, the resiliency of the city seems very abstract. But if we are talking about cities responding to natural disasters quickly and effectively, then it will be clear what we suppose to do."
  },
  {
    "objectID": "WEEK4.html#sar",
    "href": "WEEK4.html#sar",
    "title": "4  WEEK4",
    "section": "4.1 SAR",
    "text": "4.1 SAR"
  },
  {
    "objectID": "WEEK5.html",
    "href": "WEEK5.html",
    "title": "5  WEEK5",
    "section": "",
    "text": "Google Earth Engine GEE is a cloud platform provided by Google for online visual computing and analysis processing of a large number of global-scale earth science data (especially satellite data). The platform has access to satellite imagery and other Earth observation data databases and sufficient computing power to process these data.\nIn layman’s terms, Google earth engine can process and analyze remote sensing data (or other earth data) online without downloading the data to your computer for processing. We just need to download the final result to our computer.\nThe image below shows the Google Earth Engine workspace\n\n\n\nGoogle Earth Engine workspace"
  },
  {
    "objectID": "WEEK5.html#how-to-use-it",
    "href": "WEEK5.html#how-to-use-it",
    "title": "5  WEEK5",
    "section": "5.2 How to use it?",
    "text": "5.2 How to use it?\nHere is the tutorial book\nFirst of all, the two most fundamental geographic data structures in the Earth Engine, Image and Feature, correspond to raster and vector data types respectively. An image consists of a dictionary of bands and attributes. Features consist of a Geometry and an attribute dictionary. A collection of images (e.g. a time series of images) is represented by ImageCollection. A collection of features is represented by FeatureCollection.\nAnd I think the most of time we use GEE is to deal with RS image, so we will focus on the Image here.\n\nLoad data\n\nBy cliking the search for data area, and then the browse data catalogue. A page will pop up, including all remote sensing image data that can be called. These Earth Engine’s public data are classified according to Climate and Weather, Imagery, Geophysical and other categories. You can choose different remote sensing image products according to your needs. This page also provides sample code for importing data.\nFor Landsat data, after selecting the specified Landsat satellites, (here take Landsat 8 Level 2, Collection 2, Tier 2 as an example) the sample code given by Explore in Earth Engine is:\nvar dataset = ee.ImageCollection('LANDSAT/LC08/C02/T2_L2')\n    .filterDate('2021-05-01', '2021-06-01');\n\n// Applies scaling factors.\nfunction applyScaleFactors(image) {\n  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  var thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0);\n  return image.addBands(opticalBands, null, true)\n              .addBands(thermalBands, null, true);\n}\n\ndataset = dataset.map(applyScaleFactors);\n\nvar visualization = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n  min: 0.0,\n  max: 0.3,\n};\n\nMap.setCenter(-83, 24, 8);\n\nMap.addLayer(dataset, visualization, 'True Color (432)');\nAmong them, ImageCollection indicates that a set of image data is imported, and the time series is from 2021-05-01 to 2021-06-01. applyScaleFactors is a function used to choose a reasonable processing scale.\n\n\n\n\n\n\nscale factor\n\n\n\nA scale factor must be applied to both Collection 1 and Collection 2 Landsat Level-2 surface reflectance and surface temperature products before using the data. GEE provides Landsat Collection 2，its surface reflectance has a scale factor of 0.0000275 and an additional offset of -0.2 per pixel\n\n\nAnd we need to add the data to our map so we can see them, so we call Map.addLayer.\nRunning the code we get the result shown below.\n\n\n\nresult\n\n\nWe can also filter the data more finely, such as selecting a single image. We can use this website to find out the Path and Row of Landsat image for a area.Using path like LANDSAT/LC08/C02/T1_L2/LC08_146040_20211127 to search the single image and load it. Here it means the path 146 and 40th row. And we can find out the specific location of this image in the map.\n\n\n\nConvert Path/Row - Lat/Long\n\n\n\nProcessing the image\n\nWe can do a lot of work on GEE, for example the band math. Here is an example for calculating Normalized Difference Vegetation Index (NDVI) value.\n\n// This function gets NDVI from Landsat 5 imagery.\nvar getNDVI = function(image) {\n  return image.normalizedDifference(['B4', 'B3']);\n};\n\n// Load two Landsat 5 images, 20 years apart.\nvar image1 = ee.Image('LANDSAT/LT05/C02/T1_TOA/LT05_044034_19900604');\nvar image2 = ee.Image('LANDSAT/LT05/C02/T1_TOA/LT05_044034_20100611');\n\n// Use false color combination to visualize the map\nvar vizParams = {bands: ['B4', 'B3', 'B1'], min: 0.02, max: 0.4, gamma: 1.3};\n\nMap.addLayer(image1,vizParams, 'image1 Landsat 5 false color');\nMap.addLayer(image2,vizParams, 'image2 Landsat 5 false color');\n\n\n\n\n\n\nNDVI image in 1990\n\n\n\n\n\n\n\nNDVI image in 2010\n\n\n\n\n\nThe NDVI value and the difference between NDVI images 20 years can be computed\n\n// Compute NDVI from the scenes.\nvar ndvi1 = getNDVI(image1);\nvar ndvi2 = getNDVI(image2);\n// Compute the difference\nvar ndviDifference = ndvi2.subtract(ndvi1);"
  },
  {
    "objectID": "WEEK5.html#application",
    "href": "WEEK5.html#application",
    "title": "5  WEEK5",
    "section": "5.3 Application",
    "text": "5.3 Application\n\nInterpreting the image\n\nClassification can be done through GEE too, here is an example of Supervised Classification based on the tutorial Interpreting an Image Classification\nFirst we load the image of the city Milan to get a scene\n// Create an Earth Engine Point object over Milan.\nvar pt=ee.Geometry.Point([9.453, 45.424]);\n\n// Filter the Landsat 8 collection and select the least cloudy image.\nvar landsat=ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n   .filterBounds(pt)\n   .filterDate('2019-01-01', '2020-01-01')\n   .sort('CLOUD_COVER')\n   .first();\n\n// Center the map on that image.\nMap.centerObject(landsat, 8);\n\n// Add Landsat image to the map.\nvar visParams={\n   bands: ['SR_B4', 'SR_B3', 'SR_B2'],\n   min: 7000,\n   max: 12000\n};\nMap.addLayer(landsat, visParams, 'Landsat 8 image');\nAnd then using the Geometry Import function (point drawing), we click the pixel of image to import named forest, developed, water, and herbaceous type of points as FeatureCollection. Each class of points constitute a layer so we will have 4 different layers: forest, water, developed and herbaceous. These points will be our training data.\n\n\n\n\n\n\nCreate Feature Collection\n\n\n\n\n\n\n\n4 Different Layers/Classes\n\n\n\n\n\nUsing the code below, we firstly combine all the training feature collections into one. And then,we select the bands whose information will be used in classification.\n// Combine training feature collections.\nvar trainingFeatures=ee.FeatureCollection([\n   forest, developed, water, herbaceous\n]).flatten();\n\n// Define prediction bands.\nvar predictionBands=[\n   'SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7',\n   'ST_B10'\n];\n\n// Sample training points.\nvar classifierTraining=landsat.select(predictionBands)\n   .sampleRegions({\n       collection: trainingFeatures,\n       properties: ['class'],\n       scale: 30\n   });\nNow the training data is ready, we need to choose a classifier. Here we use the CART classifier. The principle of this classifier will be talked in my later diary. The code in GEE is quite simply. We call the ee.Classifier.smileCart() function, and state the aim of classification is related to the class of each pixel.\n// Train a CART Classifier.\nvar classifier=ee.Classifier.smileCart().train({\n   features: classifierTraining,\n   classProperty: 'class',\n   inputProperties: predictionBands\n});\n\nAfter building the classifier, we can classify our image and visualize the results in a new map.\n// Classify the Landsat image.\nvar classified=landsat.select(predictionBands).classify(classifier);\n\n// Define classification image visualization parameters.\nvar classificationVis={\n   min: 0,\n   max: 3,\n   palette: ['589400', 'ff0000', '1a11ff', 'd0741e']\n};\n\n// Add the classified image to the map.\nMap.addLayer(classified, classificationVis, 'CART classified');\nThe figure below shows the classification results. However, the classification results are not ideal. Compared to the original image, we can see that the a lot of pixel around the forest are misclassified to water. and the class of developed which stands for the built area in cities are not well identified. In the contract, the classified herbaceous area is much lager than it actually is.\n\n\n\nClassification Results\n\n\nI am not sure about the reason… but if my code is correct, then maybe due to the sample points I collected is not enough, or the results of my visual interpretation are not very accurate…"
  },
  {
    "objectID": "WEEK5.html#reflections",
    "href": "WEEK5.html#reflections",
    "title": "5  WEEK5",
    "section": "5.4 Reflections",
    "text": "5.4 Reflections\n\nThis week we learned how to process remote sensing images in Google Earth Engine. This is a very advanced technology that can save us a lot of time for downloading data. But since this week’s class was affected by the strike, I didn’t have a good grasp of the use of GEE. But I still made some attempts.\nFirst of all, I think GEE can help us clearly list remote sensing products, which is very helpful. For me, I often struggle with which remote sensing product to use. The data catalog of GEE classifies the main application of the product, which is very helpful. In addition, I also think that the working area of GEE can automatically display web maps, which greatly facilitates our understanding of remote sensing images.\nSecondly, about the code of GEE. Generally speaking, I think the codes of many functions are well packed and very convenient to use. But we still need to have a good understanding of the fundamental geographic data structures of GEE. And GEE’s workspace also allows us to process data interactively, such as drawing elements with the mouse and storing them in layers. This is used in our image classification operation. At present, I can only implement this part by referring to the tutorial document.\nFinally, in this week’s practice, the supervised classification I tried showed a very bad results. I preliminary analyzed that it has something to do with my sample point selection. Besides, due to the limited understanding of classifiers at present, I hope that I will be more clear about it after week 6 and week 7’s study."
  },
  {
    "objectID": "WEEK6.html",
    "href": "WEEK6.html",
    "title": "6  WEEK6",
    "section": "",
    "text": "Image classification is the process of categorizing and labeling groups of pixels or vectors within an image based on specific rules (Karbhari et al. 2009). And in remote sensing, image classification is the process of assigning land cover classes to pixels. For example, classes include water, urban, forest, agriculture, and grassland.\n\n\n\nImage Classification (source)\n\n\nThe type of classification method:\n\nUnsupervised image classification\nSupervised image classification\nObject-based image analysis\n\nThe first two are very common approaches, they are in the same definition as in machine learning. We will talk about some of the classical methods in this week. For the Object-based image analysis, it groups pixels into representative vector shapes with size and geometry. It is quite effective in some cases and we will introduce it in the next week.\n\n\n\nClassification and Regression Trees (CART) algorithm is one of the popular Decision Tree and it is a Supervised image classification. It is an umbrella word that refers to the following types of decision trees:\n\nClassification Trees:attempt to predict a class label. In other words, classification is used for problems where the output (target variable) takes a finite set of values, e.g., whether it will rain tomorrow or not.\nRegression trees: used to predict a numerical label. This means your output can take an infinite set of values, e.g., a house price.\n\nThe structure of a decision tree consists of three main parts: Root nodes, Internal Nodes and Leaf Nodes. The nodes are split into subnodes on the basis of a threshold value of an attribute.\nFor Classification Trees\nThe CART algorithm does that by searching for the best homogeneity for the subnodes, with the help of the Gini Index criterion.\n\n\n\n\n\n\nNote\n\n\n\nGini Impurity is a measurement of the likelihood of an incorrect classification of a new instance of a random variable. It is a measurement used to build Decision Trees to determine how the features of a dataset should split nodes to form the tree.\n     Gini impurity= 1-(probability of yes)^2-(the probability of no)^2\n\n\nSo basically, when training the model, CART builds a process to subset the data, this means different data of categories will be aligned to different nodes(subset data group). And when it makes the finally decision (for example the impure = 0), stopping splitting data into smaller chunks and naming the category.\nFor Regression Trees\nThe main question is also about how far (deep) do we run a decision tree? When should we stop adding new nodes? As the data is numeric now, we introduce the Tree score.\nLike all other machine learning algorithms, it is important to avoid overfitting. In Decision Tree, the way we weaken the overfitting is called PRUNING (removing the leaves).\n\n\n\n\n\n\nNote\n\n\n\n     Tree score = SSR(sum of squared residuals) + tree penalty (alpha) * T (number of leaves)\n\n\nThe lower the Tree score is, the better the classifier will be. So if when we add two new leaves (node G and node H) grow from a existing leave (node F), and then the value of Tree score increases. In this case, we should not creating new node node G and node H.\n\n\n\nRandom forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.The fundamental concept behind random forest is a simple but powerful one — the wisdom of crowds.\nSo, it basically a vote machine. Each decision tree will provides a classification/regression result, and the most votes wins. In this case, we hope that we have different trees in the forest, this is just like we do not want to a person votes twice. So, an ensemble method is needed in order to combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.\nThis means we can literally use different data to train trees.\n\n\n\n\n\n\nWhat is Bagging?\n\n\n\nBootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method. It generates additional data for training from the dataset. This is achieved by random sampling with replacement from the original dataset. Sampling with replacement may repeat some observations in each new training data set. Every element in Bagging is equally probable for appearing in a new dataset.\n\n\n\n\n\nthe Idea of Bagging source\n\n\nOr we can choose different group of attributes to build decision tree. For example, to classify whether a watermelon is good or not, we can consider from its color, weight, whether the melon is bent or not, whether the sound of slapping a watermelon is crisp and clear or not, etc. We can use the first 3 attributes to train a classifier and then the next 3 …\n\n\n\n\n\n\nAttribute Selection Measures\n\n\n\nWhen we are building a decision tree and there are a lot of attributes of the object, then how can be choose wisely in order to increase our classification accuracy? If we follow a random approach, it may give us bad results with low accuracy. So, here is some criteria which will calculate values for every attribute.\nEntropy, Information gain, Gini index, Gain Ratio, Reduction in Variance Chi-Square\nThe values are sorted, and attributes are placed in the tree by following the order i.e, the attribute with a high value(in case of information gain) is placed at the root. For each metric, its defination can be found here.\n\n\n\n\n\nHere we introduce another well-used machine learning algorithms: Support Vector Machine (SVM)\n\n\n\nthe Objective of the SVM source\n\n\nAs we can see in the figure, the objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space (N — the number of features) that distinctly classifies the data points. The left picture shows there are quite a few planes can seperate the two classes of data. However, what we want is one with the maximum margin (the right picture).\nLike other algorithms, we can build a cost function to find the convergence point where we can have the maximum margin. In SVM, we call it Hinge loss function.\nIn addition, hyperparameters like C and Gamma (or Sigma) control SVM wiggle."
  },
  {
    "objectID": "WEEK6.html#application",
    "href": "WEEK6.html#application",
    "title": "6  WEEK6",
    "section": "6.2 Application",
    "text": "6.2 Application\n\nRecognize the Land Use and Land Cover\n\nClassification enables us to obverse land use and land cover distribution in cities, the classified results can be applied in many scenarios. For example, detecting the urban sprawl, monitoring natural disasters, evaluating the urban greenspace rate, etc. Basically, it help us to explore the information in imagery. Here is an application study by Svoboda et al. (2022)\nLCLUC program is one of the most important sources of information on the development of global environmental change. This study tried to build a Sentinel-2 data classification on Google Earth Engine to achieve Land use, land-use change and forestry (LULUCF) detection program. The study conducted several experiments to determine the best values for the Number of Trees, Variables per Split and Bag Fraction parameters.\nFigures below shows its workflow and the classification results\n\n\n\nWorkflow\n\n\n\n\n\nClassification Results\n\n\n\nEvaluate the Importance Level of Attributes\n\nAs we said before, when constructing a decision tree, it is important to decide which attribute is considered at the root. We can use quite a few indices to evaluate such as Entropy, Information gain, Gini index… and fortunately, most of the decision tree and random forest algorithms will help us to do it. This means we can tell the importance sequence from the constructed tree. The first one being considered is the one of most importance.\nHere is an example of using this kind of idea to compute the importance degree of each index and to obtain the flood hazard map (Farhadi et al. 2021). In this study, 11 risk indices (Elevation (El), Slope (Sl), Slope Aspect (SA), Land Use (LU), Normalized Difference Vegetation Index (NDVI), Normalized Difference Water Index (NDWI), Topographic Wetness Index (TWI), River Distance (RD), Waterway and River Density (WRD), Soil Texture (ST]), and Maximum One-Day Precipitation (M1DP)) were provided to predict the flood risk. And the author took Random Forest as a robust data mining technique, to define the Index Importance Degree of each attributes.\nThe methodology in details can be found in the paper. And according to the results, the WRD index containing about 23.8 percent of the total risk has the greatest impact on floods. The figure below shows the ranking of all the indices.\n\n\n\nRelative importance degree of indices"
  },
  {
    "objectID": "WEEK6.html#reflection",
    "href": "WEEK6.html#reflection",
    "title": "6  WEEK6",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\n\nThis week we mainly studied the application of machine learning methods in remote sensing image processing, with a special focus on the application of decision trees and random forests in classification tasks. Both methods are widely used, because that they are very easy to understand and interpret because they can be visualized.This type of algorithm is like a human being when making a choice, by judging different characteristics, making a choice step by step, and finally reaching the terminal node. On the basis of the decision tree, the random forest is like a group of people making choices together, and then get the final decision in the form of voting.\nHowever, although their rationale is simple, there are still many factors that need to be considered in the process of building a decision tree. For example, how to determine the depth of the tree. Sometimes decision tree learners may over complicate the tree and not generalize the data well. This problem is known as overfitting.In addition，They don’t have very good stability. One small variation in data may cause the tree to completely change.\nFinally, the two application examples in this learning diary also prove that the combination of GEE and machine learning methods is very effective. When there is a lot of data to be processed, or when the algorithm has a certain complexity, using the GEE platform is a choice."
  },
  {
    "objectID": "WEEK6.html#reference",
    "href": "WEEK6.html#reference",
    "title": "6  WEEK6",
    "section": "6.4 Reference",
    "text": "6.4 Reference\n        Svoboda J, Štych P, Laštovička J, Paluba D, Kobliuk N. Random Forest Classification of Land Use, Land-Use Change and Forestry (LULUCF) Using Sentinel-2 Data—A Case Study of Czechia. Remote Sensing. 2022; 14(5):1189. https://doi.org/10.3390/rs14051189\n        Farhadi H, Najafzadeh M. Flood Risk Mapping by Remote Sensing Data and Random Forest Technique. Water. 2021; 13(21):3115. https://doi.org/10.3390/w13213115"
  },
  {
    "objectID": "WEEK7.html#application",
    "href": "WEEK7.html#application",
    "title": "7  WEEK7",
    "section": "7.2 Application",
    "text": "7.2 Application\n\nImage classification\n\nBasically, the Object based image analysis is seen as a way to improve the classification accuracy, especially in those area the composition of land is complex. For example, Krause et al (2021) improved the mapping of coastal slt marsh habitat change using object-based image analysis of high-resolution aerial imagery. The study results shows that the suitability of high-resolution imagery for the detection of open water features. And it also demonstrates the advantage of OBIA.\n\n\n\nSchema of geospatial imagery processing and classification workflow\n\n\nHere is a workflow of Krause and others’ workflow. After the classification, a change map was carried out to illustrate the trend of salt marsh in the study area. From my point of view, the OBIA integrated the pixel with this feature, so the classified object is bigger than pixel. That could help us observe the different between before and now.\nGhorbanzadeh et al (2022) also applied OBIA to the landslide detection. Intuitive annotation of landslides from satellite imagery is based on distinct features rather than individual pixels. Hence, the study examed the feasibility of the integration framework of a DL model with rule-based object-based image analysis (OBIA) to detect landslides.\n\nConfusion Matrix\n\nIn Ghorbanzadeh’s research, the landslide detection results were validated by measuring the number of pixels allocated as true positive (TP), false positive (FP), and false negative (FN). The picture blow shows the assessment results.\n\n\n\nAccuracy Assessment\n\n\nWe know that true positive (TP) means an outcome where the model correctly predicts the positive class. So we can compare the light green area to know which algorithm can recognize the most of landslide area. And the same way for FP and FN."
  },
  {
    "objectID": "WEEK7.html#reflection",
    "href": "WEEK7.html#reflection",
    "title": "7  WEEK7",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\n\nThis week we learn another type of classification method in RS, the Object-based image analysis (OBIA). It differs from the pixel-based image analysis as the OBIA pay attention to a group of pixels with the same feature. It is similar to the way people interpret the image visually. Besides, by aggregating the pixel into objects, information beyond the colour can be obtained when classification. Further, OBIA can also avoid the pepper image in the end, which is definitely good for the next analysis.\nHowever, using OBIA means that you need segment the image before carrying out classification algorithm. Therefore, the quality of segmentation well affect the final classification accuracy. Liu et al (2010) stated that there are two main disadvanteges of OBIA, that is, (1) segmentation accuracies decrease with increasing segmentation scales and the negative impacts of under-segmentation errors become significantly large at large scales and (2) there are both advantages and limitations in using object-based classification, and their trade-off determines the overall effect of object-based classification, which is dependent on the segmentation scales.\nSo, in my opinion, OBIA may improve the classification results in most of cases, but the previous segmentation also increase the risk of mis-classification.\nAnd for the accuracy assessment, I think the Confusion Matrix is a popular and helpful way for people to test the classification results. TF,TP,NF,NP can help us analyse the results in different ways. For example, if we want to recognize the category correctly as much as possible, we should choose the model with the highest TF value. However, if we want to minimize the probability of identifying healthy people as sick people, we need to focus on the FP value."
  },
  {
    "objectID": "WEEK7.html#reference",
    "href": "WEEK7.html#reference",
    "title": "7  WEEK7",
    "section": "7.4 Reference",
    "text": "7.4 Reference\n        Krause, Johannes R., Autumn J. Oczkowski, and Elizabeth Burke Watson. “Improved mapping of coastal salt marsh habitat change at Barnegat Bay (NJ, USA) using object-based image analysis of high-resolution aerial imagery.” Remote Sensing Applications: Society and Environment 29 (2023): 100910.\n        Ghorbanzadeh, Omid, et al. “Landslide detection using deep learning and object-based image analysis.” Landslides 19.4 (2022): 929-939.\n        Liu, Desheng, and Fan Xia. “Assessing object-based classification: advantages and limitations.” Remote sensing letters 1.4 (2010): 187-194."
  },
  {
    "objectID": "WEEK7.html",
    "href": "WEEK7.html",
    "title": "7  WEEK7",
    "section": "",
    "text": "OBIA\n\n\nThe classification methods we summarized last week are pixel-based methods. The enormous information of the spectrum can help us tell pixels in different categories. However, there are also some disadvantages of that kinds of methods. One of the most significant is the production of pepper images. In other words, when doing the classification, the pixels are independent to each other, so sometimes the distribution of categories can be fragmented which is not good for spatial analysis.\nTo deal with this, we need a method that well involved in the structure of the image, consider the similarity (homogeneity) or difference (heterogeneity) of the cells.\nObject based image analysis (OBIA) is a way to classification the image. It includes 2 part: segmentation and classification. The first part aims at breaking the image up into objects representing land-based features. And the following one is about classifying those objects using their shape, size, spatial and spectral properties.\nSo, segmenting the image into objects is the first step.Here we introduce SLIC (Simple Linear Iterative Clustering) Algorithm for Superpixel generation.\n\n\n\n\n\n\nWhat is a Superpixel?\n\n\n\nA superpixel can be defined as a group of pixels that share common characteristics (like pixel intensity ). Superpixels can provide more information than pixels as pixels belongs to a given superpixel share similar visual properties.\n\n\nSLIC generates superpixels by clustering pixels based on their color similarity and proximity in the image plane. This procedure of the algorithm can be summarized as shown in the picture below:\n\n\n\nSLTC algorithm\n\n\n\n\n\nIt is important to estimate the accuracy of the classification results. No matter what kind of method we use, we have a same way to do the accuracy assessment.\nThe binary confusion matrix is widely use in all kinds of binary classification problem. It use a 2 x 2 matrix to document the number of cases in different relationship of prediction and actual results. So, this matrix can help us analysis the prediction quality from various perspective.\n\n\n\nConfusion Matrix (Source)\n\n\nThe picture shows what a confusion matrix looks like. T stands for Ture maens that the predicted result is as same as the actual condition, F for False and vice versa.\nTherefore,\n\nA true positive (TP) is an outcome where the model correctly predicts the positive class.\nA true negative (TN) is an outcome where the model correctly predicts the negative class.\nA false positive (FP) is an outcome where the model incorrectly predicts the positive class.\nA false positive (FN) is an outcome where the model incorrectly predicts the negative class.\n\nMoreover, by counting the sum of the rows and columns of the matrix, the different types of accuracy can be known. For example, the PA Producer accuracy, UA User’s accuracy and OA the (overall) accuracy.\nIn the remote sensing image classification scenario, most of time we have more than 2 categories, so the matrix maybe 3 x 3 or 4 x 4. But no matter what scale the matrix is, the accuracy assessment are in similar way.\nThe kappa coefficient is an indicator used for consistency testing and can also be used to measure the effectiveness of a classification. The kappa coefficient is calculated based on the confusion matrix and takes values between -1 and 1, usually greater than 0. The equation and detailed information about kappa coefficient can be find here.\n\n\n\nThrough all the content in week 6 and 7, we can get a conclusion of the basic process of remote sensing clasification, that is:\n\nclass definition\npre-processing\ntraining\npixel assignment\naccuracy assessment\n\nHowever, there maybe some problems, such as “Spatial autocorrelation between training and test sets” And the best approach is, the (spatial) cross validation.\nCross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. Therefore, if we are talking about spatial cross validation, it means we use different part of spatial data for training and testing dataset."
  },
  {
    "objectID": "WEEK8.html",
    "href": "WEEK8.html",
    "title": "8  WEEK8",
    "section": "",
    "text": "Urban Heat Island (Source)\n\n\n\nWhat is urban heat island?\n\nUrban heat islands means that the metropolitan places are hotter than their outlying areas. This is because the construction of buildings, pavements and the usage of car results in little green area in the city. With a decreased amount of vegetation, cities lose the shade and evaporative cooling effect of trees. Besides, as dark surfaces absorb significantly more solar radiation, the metropolitan places receive more while lose back less energy than rural area. And the high density of population means more heat created by human activities like running engines and air conditioners. Last, the pollution in city increase the temperature as many forms of pollution change the radioactive properties of the atmosphere.\nUrban heat islands is associated with the climate change. According to Dodman et al.(2022), climate change is not a cause but an amplifier of the urban heat island effect. According to a report of EPA, in many areas of the U.S., steadily increasing warming trends are intensifying already higher temperatures in heat island areas. This continued warming is expected to worsen heat islands in the future.\n\n\n\nAnnual Heat Wave Frequency and Season Length in 50 Large U.S. Cities, 1961–2019 (Source)\n\n\nThe maps show that the unusually hot days in USA have steadily increased in most of the cities. This may cause serious health problem of citizens, especially to those who often work outside. When people are exposed to extreme heat, they can suffer from potentially deadly illnesses, such as heat exhaustion and heat stroke. Hot temperatures can also contribute to deaths from heart attacks, strokes, and other forms of cardiovascular disease. Extreme heat events strain high-risk populations disproportionately (Sarofim et al., 2016)\n\nBeat The Heat Handbook\n\nSo, what we could do to deal with Urban heat island? We can easily come up with the idea like planting more trees, reducing the emission of carbon dioxide, mitigating the pollution and so on. However, the most significant thing is about how to achieve them? Can urban policy be helpful?\nBeating the Heat: A Sustainable Cooling Handbook for Cities is a guide offers planners an encyclopaedia of proven options to help cool cities. According to the book, the whole-system approach to optimally address urban cooling can be divided into 3 part. 1) Reduce heat at the urban scale, 2) Reduce cooling needs in buildings, 3) Serve cooling needs in buildings efficiently.And in each part, it provides some suggertion and case examples.\nFor instance, in Seoul, Republic of Korea, An effort to restore the Cheonggyecheon stream that runs through the city replaced 5.8 kilometres of elevated expressway that covered the stream with a mixed-use waterfront corridor. The waterfront corridor decreased temperatures by 3.3°C to 5.9°C compared to a parallel road a few blocks away.\n\n\n\nThe waterfront corridor in Seoul\n\n\nIt is useful for government and other organization to know, what is the good way to deal with urban heat island. However, the policies can only give you conceptions, the vague direction. This kind of handbook can not help us understand how to execute. How we can measure the heat index? What specific software or hardware we need to use? Are there any planning rules against what we try to do? How much and how long it will take to achieve the idea?\n\nApproaching projects\n\nReferring to a specific policy, to execute a project we need to know:\nFirst, what is our research question, what kind of problem do we need to address? More particular, which index do we need to measure?\nSecond, what data should be included for our metrics? And for the EO data, which scale or resolution are the most appropriate?\nAnd then, what is our methodology? What is the criteria for the results….\n\n\n\nRemote Sensing data can be useful to explore temperature across urban areas, here we try to use MODIS data to do some pratice.\n\nMODIS\n\nModerate Resolution Imaging Spectroradiometer (MODIS), is a key instrument aboard the Terra (originally known as EOS AM-1) and Aqua (originally known as EOS PM-1) satellites. Terra MODIS and Aqua MODIS are viewing the entire Earth’s surface every one to two days, obtaining data in 36 spectral bands with wavelengths ranging from 0.4 to 14.385 μm (Li et al. 2019)\n\n\n\nMODIS in GEE\n\n\nHere we use Google Earth Engine to do upload MODIS data. You can find MODIS after cliking the Browse the data catalog. in the exploration page, a range of MODIS products can be found.\n\n\n\nMODIS in GEE 2\n\n\nWe chose MOD11A2.061 Terra Land Surface Temperature and Emissivity 8-Day Global 1km, and using the sample code blow, the data was uploaded successfully.\nvar dataset = ee.ImageCollection('MODIS/061/MOD11A2')\n                  .filter(ee.Filter.date('2018-01-01', '2018-05-01'));\nvar landSurfaceTemperature = dataset.select('LST_Day_1km');\nvar landSurfaceTemperatureVis = {\n  min: 14000.0,\n  max: 16000.0,\n  palette: [\n    '040274', '040281', '0502a3', '0502b8', '0502ce', '0502e6',\n    '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef',\n    '3be285', '3ff38f', '86e26f', '3ae237', 'b5e22e', 'd6e21f',\n    'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08', 'ff500d',\n    'ff0000', 'de0101', 'c21301', 'a71001', '911003'\n  ],\n};\nMap.setCenter(6.746, 46.529, 2);\nMap.addLayer(\n    landSurfaceTemperature, landSurfaceTemperatureVis,\n    'Land Surface Temperature');\n\n\n\n\nMODIS in GEE 3\n\n\nWith the help of Vector data , we can focus on a specific area. Here we choose Beijin City. (The code blow is just some of the essential part)\n\nvar dataset = ee.FeatureCollection(\"FAO/GAUL/2015/level1\");\n\nvar dataset_style = dataset.style({\n  color: '1e90ff',\n  width: 2,\n  fillColor: '00000000',  // with alpha set for partial transparency\n//  lineType: 'dotted',\n//  pointSize: 10,\n//  pointShape: 'circle'\n});\n\nMap.addLayer(dataset, {}, 'Second Level Administrative Units_1');\n\nvar Beijing = dataset.filter('ADM1_CODE == 899');\n\n\nMap.addLayer(Beijing, {}, 'Beijing');\nAnd we use 2 types of MODIS data,MYD11A1.006 Aqua Land Surface Temperature and Emissivity Daily Global 1km, and MOD11A1.006 Terra Land Surface Temperature and Emissivity Daily Global 1km\nWe use the MODISscale function because the MODIS data needs to have the scale factor applied\n\nfunction MODISscale(image) {\n  var temp = image.select('LST_.*').multiply(0.02).subtract(273.1);\n  return image.addBands(temp, null, true)\n}\n\nvar MODIS_Aqua_day = ee.ImageCollection('MODIS/061/MYD11A1')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filter(ee.Filter.calendarRange(5, 9,'month'))\n  .select('LST_Day_1km')\n  .map(MODISscale)\n  .filterBounds(Beijing);  // Intersecting ROI;\n\nvar MODIS_Terra_day = ee.ImageCollection('MODIS/061/MOD11A1')\n  .filterDate('2022-01-01', '2022-10-10')\n  .filter(ee.Filter.calendarRange(5, 9,'month'))\n  .filterBounds(Beijing)  // Intersecting ROI;\n  .select('LST_Day_1km')\n  .map(MODISscale);\n  \nMerge the two collections and calculate a mean summer temperature\n\nvar mean_aqua_terra = MODIS_Aqua_day.merge(MODIS_Terra_day)\n  .reduce(ee.Reducer.mean())\n  .clip(Beijing)\n\nMap.addLayer(mean_aqua_terra, landSurfaceTemperatureVis,\n    'MODIS Land Surface Temperature');\nAnd then choose the palette to show the map\n\nvar landSurfaceTemperatureVis = {\n  min: 15,\n  max: 45,\n  palette: [\n    '040274', '040281', '0502a3', '0502b8', '0502ce', '0502e6',\n    '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef',\n    '3be285', '3ff38f', '86e26f', '3ae237', 'b5e22e', 'd6e21f',\n    'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08', 'ff500d',\n    'ff0000', 'de0101', 'c21301', 'a71001', '911003'\n  ],\n};\n\nMap.addLayer(mean_aqua_terra, landSurfaceTemperatureVis,\n    'MODIS Land Surface Temperature');\n\n\n\nTemperature of Beijin\n\n\nIn GEE, we can use temperature data to do other analysis such as the time series analysis, trend analysis and heat index calculation."
  },
  {
    "objectID": "WEEK8.html#application",
    "href": "WEEK8.html#application",
    "title": "8  WEEK8",
    "section": "8.2 Application",
    "text": "8.2 Application\nTemperature data is a good product to moniture the climate in the city. It can help us understand climate change, the urban heat island, greenhouse effect and other urban problems. And also can be used in monitoring urban ecosystems, giving suggestion to urban planning.\nHere we refer some paper to have a look at how MODIS data can be use in urban study.\n\nEstimate the air surface temperature\n\nAir surface temperature (\\(T_{air}\\)) is an important parameter for a wide range of applications such as vector-borne disease bionomics, hydrology and climate change studies. These data are generally collected through observations at meteorological stations, so they have limitations such as the scale is not fine enough. And not all areas have the ability to build a meteorological stations, so some developed areas often lack such data. The use of remote sensing data can help to overcome this problem by provide estimates of surface air temperature. For example，Benali et al., (2012) accurately estimated \\(T_{max}\\), \\(T_{min}\\) and \\(T_{abg}\\) for a 10 year period based on remote sensing—Land Surface Temperature (LST) data obtained from MODIS—and auxiliary data using a statistical approach. Vancutsem et al., (2010) explored the possibility of retrieving high-resolution Ta data from the Moderate Resolution Imaging Spectroradiometer (MODIS) Ts products over different ecosystems in Africa.\n\nMonitoring agricultural drought\n\nThe impact of temperature on agriculture is very significant. Therefore, monitoring drought using remote sensing data is important for water resource planning and management to mitigate impacts on regional agriculture. Son et al.,(2012) explored the applicability of monthly MODIS normalized difference vegetation index (NDVI) and land surface temperature (LST) data for agricultural drought monitoring in LMB in the dry season from November 2001 to April 2010. The data were processed using the temperature vegetation dryness index (TVDI), calculated by parameterizing the relationship between the MODIS NDVI and LST data. Their study manifested that there is a strong correlation between TVDI and AMSR-E soil moisture data (data used for testing) calculated from remote sensing data. Therefore, it is of great significance to use remote sensing data to predict drought in areas where soil moisture measurement cannot be achieved.\n\n\n\nDrought Area"
  },
  {
    "objectID": "WEEK8.html#reflection",
    "href": "WEEK8.html#reflection",
    "title": "8  WEEK8",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\n\nThis week we took a detailed look at one type of remote sensing data—data related to temperature. This is a very important part of EO data. Because traditional temperature data needs to be collected by some observation stations, which involves expensive construction costs and labor costs. And it is impossible to achieve fine-scale collection. Remote sensing data as a kind of EO data can solve this problem well. By building related sensors on satellites, the observation of temperature can be realized, and temperature data can be provided for the whole world. This is conducive to solving various climate problems and providing assistance for agricultural production.\nBy reading the literature, taking MODIS data as an example，urban heat island (UHI), air temperature estimation/mapping (Ta estimation), soil moisture, evapotranspiration estimation, and drought monitoring/estimation were the most popular applications of MODIS LST data.This type of remote sensing data has achieved remarkable results in the fields of environment, agriculture, biology and social sciences (Phan et al., 2018). In spite of the advantages of MODIS LST data, some limitations remain, such as the cloud cover effect (included missing pixels, as well as the pixels covered by thin cloud) and the use of the MODIS data quality file (QC file)."
  },
  {
    "objectID": "WEEK8.html#reference",
    "href": "WEEK8.html#reference",
    "title": "8  WEEK8",
    "section": "8.4 Reference",
    "text": "8.4 Reference\n        Sarofim, M.C., S. Saha, M.D. Hawkins, D.M. Mills, J. Hess, R. Horton, P. Kinney, J. Schwartz, and A. St. Juliana. 2016. Temperature-related death and illness. In The Impacts of Climate Change on Human Health in the United States: A Scientific Assessment. U.S. Global Change Research Program, Washington, DC. pp. 43–68.\n        Li, Lixin, Xiaolu Zhou, and Weitian Tong, eds. Spatiotemporal analysis of air pollution and its application in public health. Elsevier, 2019.\n        Vancutsem, C., Ceccato, P., Dinku, T., & Connor, S. J. (2010). Evaluation of MODIS land surface temperature data to estimate air temperature in different ecosystems over Africa. Remote Sensing of Environment, 114(2), 449-465.\n        Benali, A., Carvalho, A. C., Nunes, J. P., Carvalhais, N., & Santos, A. (2012). Estimating air surface temperature in Portugal using MODIS LST data. Remote Sensing of Environment, 124, 108-121.\n        Son, N. T., Chen, C. F., Chen, C. R., Chang, L. Y., & Minh, V. Q. (2012). Monitoring agricultural drought in the Lower Mekong Basin using MODIS NDVI and land surface temperature data. International Journal of Applied Earth Observation and Geoinformation, 18, 417-427."
  }
]